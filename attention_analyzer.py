"""
ì–´í…ì…˜ ë¶„ì„ ëª¨ë“ˆ
AttentionAnalyzer í´ë˜ìŠ¤ - ë³´í—˜ ì¶”ì²œ ëª¨ë¸ì˜ ì–´í…ì…˜ íŒ¨í„´ ë¶„ì„ ë„êµ¬
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import AutoTokenizer, AutoModel
import pandas as pd
from typing import List, Tuple, Dict
import warnings
warnings.filterwarnings('ignore')

from config import MODEL_PATHS, ATTENTION_CONFIG


class AttentionAnalyzer:
    """ë³´í—˜ ì¶”ì²œ ëª¨ë¸ì˜ ì–´í…ì…˜ íŒ¨í„´ ë¶„ì„ ë„êµ¬"""
    
    def __init__(self, model_path: str = None):
        self.model_path = model_path or MODEL_PATHS['trained_model']
        self.tokenizer = None
        self.model = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    def load_model(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        print(f"ğŸ“‚ ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_path}")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            self.model = AutoModel.from_pretrained(self.model_path, output_attentions=True)
            self.model.to(self.device)
            self.model.eval()
            print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_path}")
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ğŸ“‚ ê¸°ë³¸ ëª¨ë¸ë¡œ ëŒ€ì²´ ì‹œë„...")
            # Fallback to pretrained model
            fallback_path = MODEL_PATHS['pretrained_model']
            self.tokenizer = AutoTokenizer.from_pretrained(fallback_path)
            self.model = AutoModel.from_pretrained(fallback_path, output_attentions=True)
            self.model.to(self.device)
            self.model.eval()
            print(f"âœ… ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {fallback_path}")
    
    def clean_token(self, token: str) -> str:
        """í† í° ì •ë¦¬ í•¨ìˆ˜"""
        cleaned = token.replace('â–', '').replace('##', '')
        if cleaned in ['[CLS]', '[SEP]', '[PAD]', '<s>', '</s>', '<pad>', '<unk>']:
            return cleaned
        if not cleaned.strip():
            return '[SPACE]'
        return cleaned
    
    def get_attention_weights(self, text: str) -> Tuple[List[str], torch.Tensor]:
        """í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì¶”ì¶œ"""
        inputs = self.tokenizer(
            text, return_tensors="pt", padding=True, 
            truncation=True, max_length=128
        ).to(self.device)
        
        raw_tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
        print(f"raw_tokens: {raw_tokens}")
        tokens = [self.clean_token(token) for token in raw_tokens]
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            attention_weights = torch.stack(outputs.attentions)
            attention_weights = attention_weights.squeeze(1)
        
        return tokens, attention_weights.cpu()
    
    def visualize_token_importance(self, text: str, layer_idx: int = -1, top_k: int = None):
        """í† í° ì¤‘ìš”ë„ ë§‰ëŒ€ ê·¸ë˜í”„"""
        if top_k is None:
            top_k = ATTENTION_CONFIG['top_k_tokens']
            
        tokens, attention_weights = self.get_attention_weights(text)
        
        if layer_idx == -1:
            layer_idx = attention_weights.shape[0] - 1
        attention = attention_weights[layer_idx].mean(dim=0)  # ëª¨ë“  í—¤ë“œ í‰ê· 
        
        cls_attention = attention[0, :].numpy()  # CLS í† í°ì˜ ì–´í…ì…˜
        
        df = pd.DataFrame({
            'token': tokens,
            'importance': cls_attention,
            'position': range(len(tokens))
        })

        df = df.sort_values('importance', ascending=False)
        
        # íŠ¹ìˆ˜ í† í° ì œì™¸
        special_tokens = ['[CLS]', '[SEP]', '[PAD]', '<s>', '</s>', '<pad>', '[SPACE]']
        df_filtered = df[~df['token'].isin(special_tokens)].head(top_k)
        
        # ì‹œê°í™”
        plt.figure(figsize=(12, 6))
        bars = plt.bar(range(len(df_filtered)), df_filtered['importance'], 
                       color=plt.cm.viridis(df_filtered['importance'] / df_filtered['importance'].max()))
        
        plt.xlabel('Tokens', fontsize=12)
        plt.ylabel('Attention Weight', fontsize=12)
        plt.title(f'Token Importance Analysis (Layer {layer_idx})\nText: "{text[:50]}..."', 
                 fontsize=14, pad=20)
        plt.xticks(range(len(df_filtered)), df_filtered['token'], rotation=45, ha='right')
        
        # ê°’ í‘œì‹œ
        for i, (bar, val) in enumerate(zip(bars, df_filtered['importance'])):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, 
                    f'{val:.3f}', ha='center', va='bottom', fontsize=9)
        
        plt.tight_layout()
        plt.show()
        
        return df_filtered
    
    def visualize_attention_heatmap(self, text: str, layer_idx: int = -1, head_idx: int = 0):
        """ì–´í…ì…˜ íˆíŠ¸ë§µ ì‹œê°í™”"""
        tokens, attention_weights = self.get_attention_weights(text)
        
        if layer_idx == -1:
            layer_idx = attention_weights.shape[0] - 1
        
        attention = attention_weights[layer_idx, head_idx].numpy()
        
        # í† í° ê¸¸ì´ ì œí•œ
        max_tokens = ATTENTION_CONFIG['max_tokens']
        if len(tokens) > max_tokens:
            tokens = tokens[:max_tokens]
            attention = attention[:max_tokens, :max_tokens]
        
        # í† í° ê¸¸ì´ ì œí•œ (í‘œì‹œìš©)
        display_tokens = [token[:6] + '..' if len(token) > 8 else token for token in tokens]
        
        plt.figure(figsize=(12, 10))
        sns.heatmap(
            attention, xticklabels=display_tokens, yticklabels=display_tokens,
            cmap='Blues', annot=False, cbar_kws={'label': 'Attention Weight'}
        )
        
        plt.title(f'Attention Heatmap\nLayer {layer_idx}, Head {head_idx}\nText: "{text[:50]}..."', 
                 fontsize=14, pad=20)
        plt.xlabel('Key Tokens', fontsize=12)
        plt.ylabel('Query Tokens', fontsize=12)
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.show()

    def analyze_query_value_attention(self, query: str, value: str):
        """ì¿¼ë¦¬ì™€ ìƒí’ˆ ì •ë³´ì˜ ì–´í…ì…˜ íŒ¨í„´ì„ ë™ì‹œì— ë¶„ì„"""
        print("ğŸ” ì¿¼ë¦¬-ìƒí’ˆ ì–´í…ì…˜ íŒ¨í„´ ë¶„ì„")
        print("=" * 60)
        
        print(f"ğŸ“ ì¿¼ë¦¬: {query[:100]}...")
        print(f"ğŸ·ï¸ ìƒí’ˆ: {value[:100]}...")
        print("-" * 40)
        
        # ì¿¼ë¦¬ ë¶„ì„
        print("ğŸ“Š ì¿¼ë¦¬ í† í° ì¤‘ìš”ë„:")
        query_analysis = self.visualize_token_importance(query)
        query_top_tokens = query_analysis['token'].head(3).tolist()
        print(f"ìƒìœ„ í† í°: {', '.join(query_top_tokens)}")
        
        # ìƒí’ˆ ë¶„ì„
        print("\nğŸ“Š ìƒí’ˆ í† í° ì¤‘ìš”ë„:")
        value_analysis = self.visualize_token_importance(value)
        value_top_tokens = value_analysis['token'].head(3).tolist()
        print(f"ìƒìœ„ í† í°: {', '.join(value_top_tokens)}")
        
        # ì¿¼ë¦¬ ì–´í…ì…˜ íˆíŠ¸ë§µ
        print("\nğŸ“Š ì¿¼ë¦¬ ì–´í…ì…˜ íˆíŠ¸ë§µ:")
        self.visualize_attention_heatmap(query, layer_idx=-1, head_idx=0)
        
        return {
            'query_analysis': query_analysis,
            'value_analysis': value_analysis,
            'query_top_tokens': query_top_tokens,
            'value_top_tokens': value_top_tokens
        }


def analyze_test_samples(test_df: pd.DataFrame, 
                        model_path: str = None,
                        sample_size: int = 3) -> Tuple['AttentionAnalyzer', List[Dict]]:
    """
    test_dfì—ì„œ ìƒ˜í”Œì„ ë½‘ì•„ì„œ ì–´í…ì…˜ ì‹œê°í™”
    
    Args:
        test_df: í…ŒìŠ¤íŠ¸ ë°ì´í„°í”„ë ˆì„ (query, value ì»¬ëŸ¼ í•„ìš”)
        model_path: ëª¨ë¸ ê²½ë¡œ
        sample_size: ë¶„ì„í•  ìƒ˜í”Œ ê°œìˆ˜
        
    Returns:
        (analyzer, results) íŠœí”Œ
    """
    print("ğŸ” í…ŒìŠ¤íŠ¸ ë°ì´í„° ì–´í…ì…˜ ë¶„ì„ ì‹œì‘...")
    print(f"ğŸ“Š test_df í¬ê¸°: {test_df.shape}")
    print(f"ğŸ“‹ ì»¬ëŸ¼ëª…: {list(test_df.columns)}")
    
    # ë¶„ì„ê¸° ìƒì„± ë° ëª¨ë¸ ë¡œë“œ
    analyzer = AttentionAnalyzer(model_path)
    analyzer.load_model()
    
    # ìƒ˜í”Œ ë°ì´í„° ì¶”ì¶œ
    sample_data = test_df.sample(n=min(sample_size, len(test_df)), random_state=42)
    
    print(f"\nğŸ“Š {len(sample_data)}ê°œ ìƒ˜í”Œ ë¶„ì„ ì‹œì‘")
    print("=" * 60)
    
    results = []
    
    for i, (idx, row) in enumerate(sample_data.iterrows()):
        query = str(row['query'])
        value = str(row['value'])
        
        print(f"\nğŸ” ìƒ˜í”Œ {i+1}/{len(sample_data)}")
        
        # ì¿¼ë¦¬-ìƒí’ˆ ì–´í…ì…˜ ë¶„ì„
        analysis_result = analyzer.analyze_query_value_attention(query, value)
        
        results.append({
            'index': idx,
            'query': query,
            'value': value,
            **analysis_result
        })
        
        print(f"âœ… ìƒ˜í”Œ {i+1} ë¶„ì„ ì™„ë£Œ\n")
    
    print("ğŸ‰ ëª¨ë“  ìƒ˜í”Œ ë¶„ì„ ì™„ë£Œ!")
    return analyzer, results


def compare_attention_patterns(analyzer: 'AttentionAnalyzer', 
                              queries: List[str], 
                              labels: List[str] = None):
    """ì—¬ëŸ¬ ì¿¼ë¦¬ì˜ ì–´í…ì…˜ íŒ¨í„´ ë¹„êµ"""
    print("ğŸ” ì–´í…ì…˜ íŒ¨í„´ ë¹„êµ ë¶„ì„")
    print("=" * 50)
    
    if labels is None:
        labels = [f"Query {i+1}" for i in range(len(queries))]
    
    all_analyses = []
    
    for i, (query, label) in enumerate(zip(queries, labels)):
        print(f"\nğŸ“ {label}: {query[:80]}...")
        analysis = analyzer.visualize_token_importance(query, top_k=5)
        
        top_tokens = analysis['token'].head(3).tolist()
        avg_importance = analysis['importance'].mean()
        
        all_analyses.append({
            'label': label,
            'query': query,
            'top_tokens': top_tokens,
            'avg_importance': avg_importance,
            'analysis': analysis
        })
        
        print(f"ìƒìœ„ í† í°: {', '.join(top_tokens)}")
        print(f"í‰ê·  ì¤‘ìš”ë„: {avg_importance:.4f}")
    
    # ë¹„êµ ìš”ì•½
    print("\nğŸ“Š ë¹„êµ ìš”ì•½:")
    print("-" * 30)
    for analysis in all_analyses:
        print(f"{analysis['label']:15}: {', '.join(analysis['top_tokens'][:2])} (í‰ê· : {analysis['avg_importance']:.3f})")
    
    return all_analyses


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    from data_preprocessing import preprocess_insurance_data
    from data_converter import InsuranceDataConverter
    from model_trainer import split_data_by_date
    
    print("ğŸ§ª AttentionAnalyzer í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    df = preprocess_insurance_data()
    sample_df = df.head(1000)  # ìƒ˜í”Œ 1000ê°œë§Œ
    
    # ë°ì´í„° ë³€í™˜
    converter = InsuranceDataConverter()
    result_df = converter.convert_dataframe(sample_df)
    
    # ë°ì´í„° ë¶„í• 
    train_df, eval_df, test_df = split_data_by_date(result_df)
    
    # ì–´í…ì…˜ ë¶„ì„ ì‹¤í–‰
    try:
        analyzer, results = analyze_test_samples(
            test_df=test_df.head(10),  # ìƒ˜í”Œ 10ê°œë§Œ
            sample_size=2
        )
        print("âœ… ì–´í…ì…˜ ë¶„ì„ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ì–´í…ì…˜ ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    print("ğŸ¯ AttentionAnalyzer í…ŒìŠ¤íŠ¸ ì™„ë£Œ") 