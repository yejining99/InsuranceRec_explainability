"""
ë³´í—˜ ì¶”ì²œ ì‹œìŠ¤í…œ Baseline ëª¨ë¸ë“¤
ë‹¤ì–‘í•œ baseline ëª¨ë¸ì„ êµ¬í˜„í•˜ì—¬ í˜„ì¬ ëª¨ë¸ê³¼ ì„±ëŠ¥ ë¹„êµ
"""

import pandas as pd
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from gensim.models import Word2Vec
from typing import List, Tuple, Dict
import logging
import warnings
warnings.filterwarnings('ignore')

from config import TRAINING_CONFIG


class BaselineModel:
    """Baseline ëª¨ë¸ì˜ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, name: str):
        self.name = name
        self.is_trained = False
    
    def fit(self, train_df: pd.DataFrame):
        """ëª¨ë¸ í•™ìŠµ"""
        raise NotImplementedError
    
    def get_similarities(self, queries: List[str], values: List[str]) -> List[float]:
        """ì¿¼ë¦¬ì™€ ê°’ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        raise NotImplementedError
    
    def evaluate(self, test_df: pd.DataFrame) -> Dict:
        """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
        queries = test_df['query'].tolist()
        values = test_df['value'].tolist()
        labels = test_df['label'].tolist()
        
        similarities = self.get_similarities(queries, values)
        
        # ìœ ì‚¬ë„ë¥¼ ì´ì§„ ë¶„ë¥˜ë¡œ ë³€í™˜ (ì„ê³„ê°’ 0.5)
        predictions = [1 if sim > 0.5 else 0 for sim in similarities]
        
        metrics = {
            'model_name': self.name,
            'accuracy': accuracy_score(labels, predictions),
            'precision': precision_score(labels, predictions, zero_division=0),
            'recall': recall_score(labels, predictions, zero_division=0),
            'f1_score': f1_score(labels, predictions, zero_division=0),
            'avg_similarity': np.mean(similarities),
            'std_similarity': np.std(similarities)
        }
        
        return metrics


class RandomBaseline(BaselineModel):
    """ëœë¤ ë² ì´ìŠ¤ë¼ì¸ - ë¬´ì‘ìœ„ ìœ ì‚¬ë„ ë°˜í™˜"""
    
    def __init__(self):
        super().__init__("Random Baseline")
        np.random.seed(TRAINING_CONFIG['random_state'])
    
    def fit(self, train_df: pd.DataFrame):
        """í•™ìŠµ ë¶ˆí•„ìš”"""
        self.is_trained = True
        logging.info(f"âœ… {self.name} ì¤€ë¹„ ì™„ë£Œ")
    
    def get_similarities(self, queries: List[str], values: List[str]) -> List[float]:
        """ë¬´ì‘ìœ„ ìœ ì‚¬ë„ ë°˜í™˜ (0~1 ì‚¬ì´)"""
        return np.random.uniform(0, 1, len(queries)).tolist()


class TfidfBaseline(BaselineModel):
    """TF-IDF ê¸°ë°˜ ë² ì´ìŠ¤ë¼ì¸"""
    
    def __init__(self, max_features: int = 5000):
        super().__init__("TF-IDF Baseline")
        self.vectorizer = TfidfVectorizer(
            max_features=max_features,
            ngram_range=(1, 2),
            stop_words=None,  # í•œêµ­ì–´ëŠ” ë³„ë„ ì²˜ë¦¬ í•„ìš”
            lowercase=True
        )
    
    def fit(self, train_df: pd.DataFrame):
        """TF-IDF ë²¡í„°ë¼ì´ì € í•™ìŠµ"""
        all_texts = train_df['query'].tolist() + train_df['value'].tolist()
        self.vectorizer.fit(all_texts)
        self.is_trained = True
        logging.info(f"âœ… {self.name} í•™ìŠµ ì™„ë£Œ (íŠ¹ì„± ìˆ˜: {len(self.vectorizer.vocabulary_)})")
    
    def get_similarities(self, queries: List[str], values: List[str]) -> List[float]:
        """TF-IDF ë²¡í„° ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        query_vectors = self.vectorizer.transform(queries)
        value_vectors = self.vectorizer.transform(values)
        
        similarities = []
        for i in range(len(queries)):
            sim = cosine_similarity(query_vectors[i], value_vectors[i])[0][0]
            similarities.append(sim)
        
        return similarities


class CountVectorizerBaseline(BaselineModel):
    """Count Vectorizer ê¸°ë°˜ ë² ì´ìŠ¤ë¼ì¸"""
    
    def __init__(self, max_features: int = 5000):
        super().__init__("Count Vectorizer Baseline")
        self.vectorizer = CountVectorizer(
            max_features=max_features,
            ngram_range=(1, 2),
            lowercase=True
        )
    
    def fit(self, train_df: pd.DataFrame):
        """Count Vectorizer í•™ìŠµ"""
        all_texts = train_df['query'].tolist() + train_df['value'].tolist()
        self.vectorizer.fit(all_texts)
        self.is_trained = True
        logging.info(f"âœ… {self.name} í•™ìŠµ ì™„ë£Œ (íŠ¹ì„± ìˆ˜: {len(self.vectorizer.vocabulary_)})")
    
    def get_similarities(self, queries: List[str], values: List[str]) -> List[float]:
        """Count ë²¡í„° ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        query_vectors = self.vectorizer.transform(queries)
        value_vectors = self.vectorizer.transform(values)
        
        similarities = []
        for i in range(len(queries)):
            sim = cosine_similarity(query_vectors[i], value_vectors[i])[0][0]
            similarities.append(sim)
        
        return similarities


class Word2VecBaseline(BaselineModel):
    """Word2Vec ê¸°ë°˜ ë² ì´ìŠ¤ë¼ì¸"""
    
    def __init__(self, vector_size: int = 100, window: int = 5, min_count: int = 1):
        super().__init__("Word2Vec Baseline")
        self.vector_size = vector_size
        self.window = window
        self.min_count = min_count
        self.model = None
    
    def _preprocess_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ê°„ë‹¨í•œ ê³µë°± ê¸°ë°˜ í† í°í™”)"""
        return text.lower().split()
    
    def _get_sentence_vector(self, sentence: str) -> np.ndarray:
        """ë¬¸ì¥ì˜ Word2Vec í‰ê·  ë²¡í„° ê³„ì‚°"""
        words = self._preprocess_text(sentence)
        vectors = []
        
        for word in words:
            if word in self.model.wv:
                vectors.append(self.model.wv[word])
        
        if vectors:
            return np.mean(vectors, axis=0)
        else:
            return np.zeros(self.vector_size)
    
    def fit(self, train_df: pd.DataFrame):
        """Word2Vec ëª¨ë¸ í•™ìŠµ"""
        all_texts = train_df['query'].tolist() + train_df['value'].tolist()
        sentences = [self._preprocess_text(text) for text in all_texts]
        
        self.model = Word2Vec(
            sentences=sentences,
            vector_size=self.vector_size,
            window=self.window,
            min_count=self.min_count,
            workers=4,
            seed=TRAINING_CONFIG['random_state']
        )
        
        self.is_trained = True
        vocab_size = len(self.model.wv.key_to_index)
        logging.info(f"âœ… {self.name} í•™ìŠµ ì™„ë£Œ (ì–´íœ˜ ìˆ˜: {vocab_size})")
    
    def get_similarities(self, queries: List[str], values: List[str]) -> List[float]:
        """Word2Vec ë²¡í„° ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        similarities = []
        
        for query, value in zip(queries, values):
            query_vec = self._get_sentence_vector(query)
            value_vec = self._get_sentence_vector(value)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            if np.linalg.norm(query_vec) > 0 and np.linalg.norm(value_vec) > 0:
                sim = cosine_similarity([query_vec], [value_vec])[0][0]
            else:
                sim = 0.0
            
            similarities.append(sim)
        
        return similarities


class PretrainedSentenceTransformerBaseline(BaselineModel):
    """ì‚¬ì „ í•™ìŠµëœ SentenceTransformer ë² ì´ìŠ¤ë¼ì¸ (fine-tuning ì—†ìŒ)"""
    
    def __init__(self, model_name: str = 'paraphrase-multilingual-MiniLM-L12-v2'):
        super().__init__(f"Pretrained {model_name}")
        self.model_name = model_name
        self.model = None
    
    def fit(self, train_df: pd.DataFrame):
        """ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ (ì¶”ê°€ í•™ìŠµ ì—†ìŒ)"""
        self.model = SentenceTransformer(self.model_name)
        if torch.cuda.is_available():
            self.model = self.model.to('cuda')
        
        self.is_trained = True
        logging.info(f"âœ… {self.name} ë¡œë“œ ì™„ë£Œ")
    
    def get_similarities(self, queries: List[str], values: List[str]) -> List[float]:
        """ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ë¡œ ì„ë² ë”© í›„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        query_embeddings = self.model.encode(queries, device=device)
        value_embeddings = self.model.encode(values, device=device)
        
        similarities = []
        for q_emb, v_emb in zip(query_embeddings, value_embeddings):
            sim = cosine_similarity([q_emb], [v_emb])[0][0]
            similarities.append(sim)
        
        return similarities


class HuggingFaceBaseline(BaselineModel):
    """HuggingFace Transformers ê¸°ë°˜ ë² ì´ìŠ¤ë¼ì¸"""
    
    def __init__(self, model_name: str = 'klue/bert-base'):
        super().__init__(f"HuggingFace {model_name}")
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    def fit(self, train_df: pd.DataFrame):
        """HuggingFace ëª¨ë¸ ë¡œë“œ"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModel.from_pretrained(self.model_name)
        self.model.to(self.device)
        self.model.eval()
        
        self.is_trained = True
        logging.info(f"âœ… {self.name} ë¡œë“œ ì™„ë£Œ")
    
    def _get_embeddings(self, texts: List[str]) -> np.ndarray:
        """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        embeddings = []
        
        for text in texts:
            inputs = self.tokenizer(
                text, return_tensors="pt", padding=True, 
                truncation=True, max_length=512
            ).to(self.device)
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                # Mean pooling
                attention_mask = inputs["attention_mask"].unsqueeze(-1)
                embeddings_tensor = outputs.last_hidden_state
                pooled = (embeddings_tensor * attention_mask).sum(1) / attention_mask.sum(1)
                embeddings.append(pooled.cpu().numpy()[0])
        
        return np.array(embeddings)
    
    def get_similarities(self, queries: List[str], values: List[str]) -> List[float]:
        """HuggingFace ëª¨ë¸ ì„ë² ë”© ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        similarities = []
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì‘ì€ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        chunk_size = 8
        
        for i in range(0, len(queries), chunk_size):
            end_idx = min(i + chunk_size, len(queries))
            
            query_chunk = queries[i:end_idx]
            value_chunk = values[i:end_idx]
            
            query_embeddings = self._get_embeddings(query_chunk)
            value_embeddings = self._get_embeddings(value_chunk)
            
            for q_emb, v_emb in zip(query_embeddings, value_embeddings):
                sim = cosine_similarity([q_emb], [v_emb])[0][0]
                similarities.append(sim)
        
        return similarities


def get_all_baseline_models() -> List[BaselineModel]:
    """ëª¨ë“  baseline ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    return [
        RandomBaseline(),
        TfidfBaseline(max_features=3000),
        CountVectorizerBaseline(max_features=3000),
        Word2VecBaseline(vector_size=100),
        PretrainedSentenceTransformerBaseline('paraphrase-multilingual-MiniLM-L12-v2'),
        PretrainedSentenceTransformerBaseline('sentence-transformers/paraphrase-multilingual-mpnet-base-v2'),
        HuggingFaceBaseline('klue/bert-base')
    ]


def run_baseline_comparison(train_df: pd.DataFrame, 
                           test_df: pd.DataFrame,
                           selected_models: List[str] = None) -> pd.DataFrame:
    """
    ëª¨ë“  baseline ëª¨ë¸ë“¤ì„ í•™ìŠµí•˜ê³  ì„±ëŠ¥ ë¹„êµ
    
    Args:
        train_df: í•™ìŠµ ë°ì´í„°
        test_df: í…ŒìŠ¤íŠ¸ ë°ì´í„°
        selected_models: ì„ íƒí•  ëª¨ë¸ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ëª¨ë“  ëª¨ë¸)
    
    Returns:
        ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ DataFrame
    """
    logging.info("ğŸ Baseline ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì‹œì‘")
    logging.info("=" * 60)
    
    all_models = get_all_baseline_models()
    
    # ì„ íƒëœ ëª¨ë¸ë§Œ í•„í„°ë§
    if selected_models:
        all_models = [model for model in all_models if model.name in selected_models]
    
    results = []
    
    for i, model in enumerate(all_models):
        logging.info(f"ğŸ”„ {i+1}/{len(all_models)}: {model.name} í‰ê°€ ì¤‘...")
        
        try:
            # ëª¨ë¸ í•™ìŠµ/ë¡œë“œ
            model.fit(train_df)
            
            # ì„±ëŠ¥ í‰ê°€
            metrics = model.evaluate(test_df)
            results.append(metrics)
            
            logging.info(f"âœ… {model.name} - F1: {metrics['f1_score']:.4f}, Acc: {metrics['accuracy']:.4f}")
            
        except Exception as e:
            logging.error(f"âŒ {model.name} í‰ê°€ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ê°’ ì¶”ê°€
            results.append({
                'model_name': model.name,
                'accuracy': 0.0,
                'precision': 0.0,
                'recall': 0.0,
                'f1_score': 0.0,
                'avg_similarity': 0.0,
                'std_similarity': 0.0,
                'error': str(e)
            })
    
    # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    results_df = pd.DataFrame(results)
    results_df = results_df.sort_values('f1_score', ascending=False)
    
    logging.info("=" * 60)
    logging.info("ğŸ“Š Baseline ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì™„ë£Œ")
    logging.info("=" * 60)
    
    return results_df


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    from data_preprocessing import preprocess_insurance_data
    from data_converter import InsuranceDataConverter
    from model_trainer import split_data_by_date
    
    print("ğŸ§ª Baseline ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    df = preprocess_insurance_data()
    sample_df = df.head(1000)  # ìƒ˜í”Œ 1000ê°œë§Œ
    
    # ë°ì´í„° ë³€í™˜
    converter = InsuranceDataConverter()
    result_df = converter.convert_dataframe(sample_df)
    
    # ë°ì´í„° ë¶„í• 
    train_df, eval_df, test_df = split_data_by_date(result_df)
    
    # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì¼ë¶€ ëª¨ë¸ë§Œ ì„ íƒ
    selected_models = [
        "Random Baseline",
        "TF-IDF Baseline",
        "Pretrained paraphrase-multilingual-MiniLM-L12-v2"
    ]
    
    # Baseline ëª¨ë¸ ë¹„êµ ì‹¤í–‰
    results_df = run_baseline_comparison(
        train_df=train_df.head(100),  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
        test_df=test_df.head(50),
        selected_models=selected_models
    )
    
    print("\n" + "=" * 60)
    print("ğŸ“Š Baseline ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼")
    print("=" * 60)
    print(results_df[['model_name', 'f1_score', 'accuracy', 'avg_similarity']].to_string(index=False))
    
    print("ğŸ¯ Baseline ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ") 