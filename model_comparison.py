"""
ë³´í—˜ ì¶”ì²œ ëª¨ë¸ ì¢…í•© ì„±ëŠ¥ ë¹„êµ
í˜„ì¬ Fine-tuned ëª¨ë¸ vs ë‹¤ì–‘í•œ Baseline ëª¨ë¸ë“¤ ì„±ëŠ¥ ë¹„êµ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import logging
from typing import Dict, List
import warnings
warnings.filterwarnings('ignore')

from model_trainer import InsuranceEmbeddingTrainer, split_data_by_date
from baseline_models import run_baseline_comparison, get_all_baseline_models
from data_preprocessing import preprocess_insurance_data
from data_converter import InsuranceDataConverter
from config import MODEL_PATHS, TRAINING_CONFIG


def setup_korean_font():
    """í•œê¸€ í°íŠ¸ ì„¤ì •"""
    plt.rcParams['font.family'] = ['DejaVu Sans', 'Malgun Gothic', 'AppleGothic']
    plt.rcParams['axes.unicode_minus'] = False


def evaluate_finetuned_model(test_df: pd.DataFrame, model_path: str = None) -> Dict:
    """Fine-tuned ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
    logging.info("ğŸ¯ Fine-tuned ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
    
    if model_path is None:
        model_path = MODEL_PATHS['trained_model']
    
    trainer = InsuranceEmbeddingTrainer()
    
    try:
        trainer.load_trained_model(model_path)
        metrics = trainer.evaluate_model_performance(test_df)
        metrics['model_name'] = 'Fine-tuned Insurance Model'
        logging.info(f"âœ… Fine-tuned ëª¨ë¸ - F1: {metrics['f1_score']:.4f}, Acc: {metrics['accuracy']:.4f}")
        return metrics
    
    except Exception as e:
        logging.error(f"âŒ Fine-tuned ëª¨ë¸ í‰ê°€ ì‹¤íŒ¨: {e}")
        return {
            'model_name': 'Fine-tuned Insurance Model',
            'accuracy': 0.0,
            'precision': 0.0,
            'recall': 0.0,
            'f1_score': 0.0,
            'avg_similarity': 0.0,
            'std_similarity': 0.0,
            'error': str(e)
        }


def comprehensive_model_comparison(train_df: pd.DataFrame, 
                                 test_df: pd.DataFrame,
                                 include_heavy_models: bool = False,
                                 finetuned_model_path: str = None) -> pd.DataFrame:
    """
    ì¢…í•©ì ì¸ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
    
    Args:
        train_df: í•™ìŠµ ë°ì´í„°
        test_df: í…ŒìŠ¤íŠ¸ ë°ì´í„°
        include_heavy_models: ë¬´ê±°ìš´ ëª¨ë¸ë“¤(HuggingFace ë“±) í¬í•¨ ì—¬ë¶€
        finetuned_model_path: Fine-tuned ëª¨ë¸ ê²½ë¡œ
    
    Returns:
        ëª¨ë“  ëª¨ë¸ì˜ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ DataFrame
    """
    logging.info("ğŸ† ì¢…í•©ì ì¸ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì‹œì‘")
    logging.info("=" * 70)
    
    all_results = []
    
    # 1. Fine-tuned ëª¨ë¸ í‰ê°€
    finetuned_result = evaluate_finetuned_model(test_df, finetuned_model_path)
    all_results.append(finetuned_result)
    
    # 2. Baseline ëª¨ë¸ë“¤ ì„ íƒ
    if include_heavy_models:
        # ëª¨ë“  baseline ëª¨ë¸ í¬í•¨
        selected_baselines = None
    else:
        # ë¹ ë¥¸ baseline ëª¨ë¸ë“¤ë§Œ ì„ íƒ
        selected_baselines = [
            "Random Baseline",
            "TF-IDF Baseline",
            "Count Vectorizer Baseline", 
            "Word2Vec Baseline",
            "Pretrained paraphrase-multilingual-MiniLM-L12-v2"
        ]
    
    # 3. Baseline ëª¨ë¸ë“¤ í‰ê°€
    baseline_results = run_baseline_comparison(
        train_df=train_df,
        test_df=test_df,
        selected_models=selected_baselines
    )
    
    # 4. ê²°ê³¼ í†µí•©
    for _, row in baseline_results.iterrows():
        all_results.append(row.to_dict())
    
    # 5. ìµœì¢… ê²°ê³¼ DataFrame ìƒì„±
    final_results = pd.DataFrame(all_results)
    final_results = final_results.sort_values('f1_score', ascending=False)
    
    logging.info("=" * 70)
    logging.info("ğŸ‰ ì¢…í•©ì ì¸ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì™„ë£Œ")
    logging.info("=" * 70)
    
    return final_results


def visualize_comparison_results(results_df: pd.DataFrame, save_path: str = None):
    """ëª¨ë¸ ë¹„êµ ê²°ê³¼ ì‹œê°í™”"""
    setup_korean_font()
    
    # ê²°ê³¼ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²˜ë¦¬
    if results_df.empty:
        logging.warning("âš ï¸ ì‹œê°í™”í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì—ëŸ¬ê°€ ìˆëŠ” ëª¨ë¸ ì œì™¸
    clean_results = results_df[~results_df['model_name'].str.contains('error', na=False)]
    
    if clean_results.empty:
        logging.warning("âš ï¸ ì •ìƒì ìœ¼ë¡œ í‰ê°€ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('ë³´í—˜ ì¶”ì²œ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ', fontsize=16, fontweight='bold')
    
    # 1. F1 Score ë¹„êµ
    ax1 = axes[0, 0]
    bars1 = ax1.bar(range(len(clean_results)), clean_results['f1_score'], 
                    color=plt.cm.viridis(np.linspace(0, 1, len(clean_results))))
    ax1.set_title('F1 Score ë¹„êµ', fontweight='bold')
    ax1.set_ylabel('F1 Score')
    ax1.set_xticks(range(len(clean_results)))
    ax1.set_xticklabels([name[:15] + '...' if len(name) > 15 else name 
                        for name in clean_results['model_name']], rotation=45, ha='right')
    
    # ê°’ í‘œì‹œ
    for i, (bar, val) in enumerate(zip(bars1, clean_results['f1_score'])):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, 
                f'{val:.3f}', ha='center', va='bottom', fontsize=9)
    
    # 2. Accuracy ë¹„êµ
    ax2 = axes[0, 1]
    bars2 = ax2.bar(range(len(clean_results)), clean_results['accuracy'],
                    color=plt.cm.plasma(np.linspace(0, 1, len(clean_results))))
    ax2.set_title('Accuracy ë¹„êµ', fontweight='bold')
    ax2.set_ylabel('Accuracy')
    ax2.set_xticks(range(len(clean_results)))
    ax2.set_xticklabels([name[:15] + '...' if len(name) > 15 else name 
                        for name in clean_results['model_name']], rotation=45, ha='right')
    
    # ê°’ í‘œì‹œ
    for i, (bar, val) in enumerate(zip(bars2, clean_results['accuracy'])):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, 
                f'{val:.3f}', ha='center', va='bottom', fontsize=9)
    
    # 3. Precision vs Recall ì‚°ì ë„
    ax3 = axes[1, 0]
    scatter = ax3.scatter(clean_results['recall'], clean_results['precision'], 
                         s=100, c=clean_results['f1_score'], cmap='viridis', alpha=0.7)
    ax3.set_xlabel('Recall')
    ax3.set_ylabel('Precision')
    ax3.set_title('Precision vs Recall', fontweight='bold')
    
    # ëª¨ë¸ ì´ë¦„ í‘œì‹œ
    for i, name in enumerate(clean_results['model_name']):
        short_name = name[:10] + '...' if len(name) > 10 else name
        ax3.annotate(short_name, 
                    (clean_results.iloc[i]['recall'], clean_results.iloc[i]['precision']),
                    xytext=(5, 5), textcoords='offset points', fontsize=8)
    
    plt.colorbar(scatter, ax=ax3, label='F1 Score')
    
    # 4. í‰ê·  ìœ ì‚¬ë„ ë¶„í¬
    ax4 = axes[1, 1]
    bars4 = ax4.bar(range(len(clean_results)), clean_results['avg_similarity'],
                    color=plt.cm.coolwarm(np.linspace(0, 1, len(clean_results))))
    ax4.set_title('í‰ê·  ìœ ì‚¬ë„ ë¹„êµ', fontweight='bold')
    ax4.set_ylabel('Average Similarity')
    ax4.set_xticks(range(len(clean_results)))
    ax4.set_xticklabels([name[:15] + '...' if len(name) > 15 else name 
                        for name in clean_results['model_name']], rotation=45, ha='right')
    
    # ê°’ í‘œì‹œ
    for i, (bar, val) in enumerate(zip(bars4, clean_results['avg_similarity'])):
        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, 
                f'{val:.3f}', ha='center', va='bottom', fontsize=9)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        logging.info(f"ğŸ“Š ì‹œê°í™” ê²°ê³¼ ì €ì¥: {save_path}")
    
    plt.show()


def print_detailed_comparison(results_df: pd.DataFrame):
    """ìƒì„¸í•œ ë¹„êµ ê²°ê³¼ ì¶œë ¥"""
    print("\n" + "=" * 80)
    print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ìƒì„¸ ê²°ê³¼")
    print("=" * 80)
    
    if results_df.empty:
        print("âŒ ë¹„êµí•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì—ëŸ¬ê°€ ìˆëŠ” ëª¨ë¸ê³¼ ì •ìƒ ëª¨ë¸ ë¶„ë¦¬
    error_models = results_df[results_df.get('error', '').notna()]
    clean_results = results_df[results_df.get('error', '').isna() | (results_df.get('error', '') == '')]
    
    if not clean_results.empty:
        print(f"ğŸ“ˆ ì •ìƒ í‰ê°€ ëª¨ë¸: {len(clean_results)}ê°œ")
        print("-" * 80)
        
        # ì„±ëŠ¥ ì§€í‘œë³„ ìˆœìœ„
        metrics = ['f1_score', 'accuracy', 'precision', 'recall', 'avg_similarity']
        
        for metric in metrics:
            if metric in clean_results.columns:
                print(f"\nğŸ† {metric.upper()} ìˆœìœ„:")
                sorted_models = clean_results.sort_values(metric, ascending=False)
                for i, (_, row) in enumerate(sorted_models.head(5).iterrows()):
                    print(f"  {i+1}. {row['model_name'][:40]:40} {row[metric]:.4f}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
        best_model = clean_results.loc[clean_results['f1_score'].idxmax()]
        print(f"\nğŸ¥‡ ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model['model_name']}")
        print(f"   F1 Score: {best_model['f1_score']:.4f}")
        print(f"   Accuracy: {best_model['accuracy']:.4f}")
        print(f"   Precision: {best_model['precision']:.4f}")
        print(f"   Recall: {best_model['recall']:.4f}")
        
        # Fine-tuned ëª¨ë¸ ìˆœìœ„
        finetuned_row = clean_results[clean_results['model_name'].str.contains('Fine-tuned', na=False)]
        if not finetuned_row.empty:
            finetuned_rank = (clean_results['f1_score'] > finetuned_row.iloc[0]['f1_score']).sum() + 1
            print(f"\nğŸ¯ Fine-tuned ëª¨ë¸ ìˆœìœ„: {finetuned_rank}/{len(clean_results)}")
            print(f"   í˜„ì¬ ëª¨ë¸ì´ {len(clean_results) - finetuned_rank}ê°œ baselineì„ ì•ì„¬")
    
    # ì—ëŸ¬ ëª¨ë¸ ë¦¬í¬íŠ¸
    if not error_models.empty:
        print(f"\nâŒ í‰ê°€ ì‹¤íŒ¨ ëª¨ë¸: {len(error_models)}ê°œ")
        print("-" * 40)
        for _, row in error_models.iterrows():
            print(f"  â€¢ {row['model_name']}: {row.get('error', 'Unknown error')}")
    
    print("=" * 80)


def quick_comparison(sample_size: int = 1000, test_size: int = 200):
    """ë¹ ë¥¸ ëª¨ë¸ ë¹„êµ (ì‘ì€ ë°ì´í„°ì…‹ ì‚¬ìš©)"""
    logging.info(f"âš¡ ë¹ ë¥¸ ëª¨ë¸ ë¹„êµ ì‹œì‘ (ìƒ˜í”Œ í¬ê¸°: {sample_size})")
    
    # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    df = preprocess_insurance_data()
    sample_df = df.head(sample_size)
    
    # ë°ì´í„° ë³€í™˜
    converter = InsuranceDataConverter()
    result_df = converter.convert_dataframe(sample_df)
    
    # ë°ì´í„° ë¶„í• 
    train_df, eval_df, test_df = split_data_by_date(result_df)
    test_df = test_df.head(test_size)
    
    # ëª¨ë¸ ë¹„êµ ì‹¤í–‰
    results = comprehensive_model_comparison(
        train_df=train_df,
        test_df=test_df,
        include_heavy_models=False  # ë¹ ë¥¸ ë¹„êµë¥¼ ìœ„í•´ ë¬´ê±°ìš´ ëª¨ë¸ ì œì™¸
    )
    
    # ê²°ê³¼ ì¶œë ¥ ë° ì‹œê°í™”
    print_detailed_comparison(results)
    visualize_comparison_results(results, save_path='quick_model_comparison.png')
    
    return results


def full_comparison(sample_size: int = None):
    """ì „ì²´ ëª¨ë¸ ë¹„êµ (ëª¨ë“  baseline í¬í•¨)"""
    logging.info("ğŸ”¥ ì „ì²´ ëª¨ë¸ ë¹„êµ ì‹œì‘ (ëª¨ë“  baseline í¬í•¨)")
    
    # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    df = preprocess_insurance_data()
    if sample_size:
        df = df.head(sample_size)
    
    # ë°ì´í„° ë³€í™˜
    converter = InsuranceDataConverter()
    result_df = converter.convert_dataframe(df)
    
    # ë°ì´í„° ë¶„í• 
    train_df, eval_df, test_df = split_data_by_date(result_df)
    
    # ëª¨ë¸ ë¹„êµ ì‹¤í–‰
    results = comprehensive_model_comparison(
        train_df=train_df,
        test_df=test_df,
        include_heavy_models=True  # ëª¨ë“  ëª¨ë¸ í¬í•¨
    )
    
    # ê²°ê³¼ ì¶œë ¥ ë° ì‹œê°í™”
    print_detailed_comparison(results)
    visualize_comparison_results(results, save_path='full_model_comparison.png')
    
    return results


if __name__ == "__main__":
    # ë¹ ë¥¸ ë¹„êµ ì‹¤í–‰
    print("ğŸš€ ë³´í—˜ ì¶”ì²œ ëª¨ë¸ ì¢…í•© ì„±ëŠ¥ ë¹„êµ")
    
    try:
        # ë¹ ë¥¸ ë¹„êµë¶€í„° ì‹œì‘
        quick_results = quick_comparison(sample_size=1000, test_size=200)
        
        # ê²°ê³¼ ì €ì¥
        quick_results.to_csv('model_comparison_results.csv', index=False, encoding='utf-8-sig')
        logging.info("ğŸ“ ê²°ê³¼ ì €ì¥: model_comparison_results.csv")
        
        print("\nğŸ¯ ë¹ ë¥¸ ë¹„êµ ì™„ë£Œ!")
        print("ì „ì²´ ë¹„êµë¥¼ ì›í•œë‹¤ë©´ full_comparison() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.")
        
    except Exception as e:
        logging.error(f"âŒ ëª¨ë¸ ë¹„êµ ì‹¤íŒ¨: {e}")
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")

    # ì „ì²´ ë¹„êµ (ëª¨ë“  baseline í¬í•¨)
    results = full_comparison(sample_size=5000)
    
    # ê²°ê³¼ ì €ì¥
    results.to_csv('full_model_comparison_results.csv', index=False, encoding='utf-8-sig')
    logging.info("ğŸ“ ê²°ê³¼ ì €ì¥: full_model_comparison_results.csv") 