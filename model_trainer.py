"""
ë³´í—˜ ì¶”ì²œ ëª¨ë¸ í•™ìŠµ ëª¨ë“ˆ
InsuranceEmbeddingTrainer í´ë˜ìŠ¤ - ì„ë² ë”© ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
"""

import pandas as pd
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer, InputExample, losses
from sentence_transformers.evaluation import BinaryClassificationEvaluator
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import logging
import random
from typing import List, Tuple, Dict
import os
from datetime import datetime

from config import MODEL_PATHS, TRAINING_CONFIG

# ë¡œê¹… ì„¤ì •
logging.basicConfig(format='%(asctime)s - %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S',
                    level=logging.INFO)


class InsuranceEmbeddingTrainer:
    """ë³´í—˜ ì¶”ì²œì„ ìœ„í•œ Embedding í•™ìŠµê¸°"""
    
    def __init__(self, 
                 model_path: str = None,
                 output_path: str = None):
        """
        Args:
            model_path: ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
            output_path: í•™ìŠµëœ ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        """
        self.model_path = model_path or MODEL_PATHS['pretrained_model']
        self.output_path = output_path or MODEL_PATHS['trained_model']
        self.tokenizer = None
        self.model = None
        self.sentence_transformer = None
        
    def load_pretrained_model(self):
        """ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ"""
        logging.info(f"Loading pretrained model from: {self.model_path}")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            self.model = AutoModel.from_pretrained(self.model_path)
            logging.info("âœ… Pretrained model loaded successfully")
        except Exception as e:
            logging.error(f"âŒ Error loading pretrained model: {e}")
            # Fallback to online model
            logging.info("Falling back to online model...")
            self.sentence_transformer = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    
    def get_embeddings_manual(self, sentences: List[str]) -> torch.Tensor:
        """ìˆ˜ë™ìœ¼ë¡œ ì„ë² ë”© ì¶”ì¶œ (ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©)"""
        if self.tokenizer is None or self.model is None:
            raise ValueError("Pretrained model not loaded. Call load_pretrained_model() first.")
        
        inputs = self.tokenizer(
            sentences, 
            padding=True, 
            truncation=True, 
            return_tensors="pt", 
            max_length=TRAINING_CONFIG['max_length']
        )
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            embeddings = outputs.last_hidden_state
            attention_mask = inputs["attention_mask"].unsqueeze(-1)
            # Mean pooling
            pooled = (embeddings * attention_mask).sum(1) / attention_mask.sum(1)
        
        return pooled
    
    def prepare_sentence_transformer(self):
        """Sentence Transformer ëª¨ë¸ ì¤€ë¹„"""
        if self.sentence_transformer is None:
            # ë¡œì»¬ ëª¨ë¸ì„ Sentence Transformerë¡œ ë³€í™˜
            logging.info("Converting pretrained model to Sentence Transformer...")
            self.sentence_transformer = SentenceTransformer(self.model_path)
            
            # GPU ì‚¬ìš© ê°€ëŠ¥ì‹œ ì´ë™
            if torch.cuda.is_available():
                self.sentence_transformer = self.sentence_transformer.to('cuda')
    
    def prepare_training_data(self, df: pd.DataFrame) -> List[InputExample]:
        """ì˜¬ë¦¬ë¸Œì˜ ë°©ì‹: Positive pairsë§Œ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ë°ì´í„° ì¤€ë¹„"""
        examples = []
        
        logging.info(f"Preparing training data with {len(df)} samples")
        
        # Label=1ì¸ positive pairsë§Œ ì‚¬ìš©
        positive_df = df[df['label'] == 1]
        
        for _, row in positive_df.iterrows():
            # Label ì—†ì´ positive pairë§Œ ìƒì„±
            # MultipleNegativesRankingLossê°€ ë°°ì¹˜ ë‚´ì—ì„œ ìë™ìœ¼ë¡œ negative sampling ìˆ˜í–‰
            examples.append(InputExample(texts=[row['query'], row['value']]))
        
        logging.info(f"Created {len(examples)} positive pairs")
        logging.info("Negative pairs will be automatically generated in-batch by MultipleNegativesRankingLoss")
        
        # ë°ì´í„° ì…”í”Œ
        random.shuffle(examples)
        return examples
    
    def create_simple_evaluator(self, eval_df: pd.DataFrame):
        """ê°„ë‹¨í•œ í‰ê°€ì ìƒì„± (ìƒê´€ê³„ìˆ˜ ëŒ€ì‹  ì •í™•ë„ ê¸°ë°˜)"""
        # í‰ê°€ ë°ì´í„°ë¥¼ binary classification í˜•íƒœë¡œ ë³€í™˜
        sentences1 = []
        sentences2 = []
        scores = []
        
        queries = eval_df['query'].tolist()
        values = eval_df['value'].tolist()
        
        # Positive pairs (ì‹¤ì œ ë§¤ì¹­)
        for query, value in zip(queries, values):
            sentences1.append(query)
            sentences2.append(value)
            scores.append(1)  # positive
        
        # Negative pairs (ëœë¤ ë§¤ì¹­)
        for i in range(min(len(queries), 20)):  # ìµœëŒ€ 20ê°œ negative
            neg_idx = (i + len(queries) // 2) % len(values)  # ë‹¤ë¥¸ ì¸ë±ìŠ¤ ì„ íƒ
            sentences1.append(queries[i])
            sentences2.append(values[neg_idx])
            scores.append(0)  # negative
        
        logging.info(f"Created binary evaluator with {len(sentences1)} pairs")
        logging.info(f"Positive: {scores.count(1)}, Negative: {scores.count(0)}")
        
        return BinaryClassificationEvaluator(
            sentences1=sentences1,
            sentences2=sentences2,
            labels=scores,
            name="insurance_binary_eval"
        )
    
    def train_model_with_safe_evaluator(self, 
                                       train_df: pd.DataFrame,
                                       eval_df: pd.DataFrame = None,
                                       epochs: int = None,
                                       batch_size: int = None,
                                       learning_rate: float = None,
                                       warmup_steps: int = None):
        """ì•ˆì „í•œ í‰ê°€ìë¥¼ ì‚¬ìš©í•œ í•™ìŠµ"""
        
        # ê¸°ë³¸ê°’ ì„¤ì •
        epochs = epochs or TRAINING_CONFIG['epochs']
        batch_size = batch_size or TRAINING_CONFIG['batch_size']
        learning_rate = learning_rate or TRAINING_CONFIG['learning_rate']
        
        # Sentence Transformer ì¤€ë¹„
        self.prepare_sentence_transformer()
        
        # í•™ìŠµ ë°ì´í„° ì¤€ë¹„ (positive pairsë§Œ)
        train_examples = self.prepare_training_data(train_df)
        train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)
        
        # Loss function ì„¤ì • (MultipleNegativesRankingLoss)
        train_loss = losses.MultipleNegativesRankingLoss(model=self.sentence_transformer)
        
        # Warmup steps ê³„ì‚°
        if warmup_steps is None:
            warmup_steps = int(len(train_dataloader) * epochs * 0.1)
        
        # ì•ˆì „í•œ í‰ê°€ì ìƒì„±
        evaluator = None
        if eval_df is not None and len(eval_df) >= 5:
            try:
                evaluator = self.create_simple_evaluator(eval_df)
                logging.info("âœ… Safe binary evaluator created")
            except Exception as e:
                logging.warning(f"âš ï¸ Failed to create safe evaluator: {e}")
                evaluator = None
        
        # ëª¨ë¸ í•™ìŠµ
        logging.info("ğŸš€ Starting training with safe evaluator...")
        logging.info(f"ğŸ“Š Training examples: {len(train_examples)}")
        logging.info(f"ğŸ“¦ Batch size: {batch_size}")
        logging.info(f"ğŸ”„ Epochs: {epochs}")
        logging.info(f"ğŸ”¥ Learning rate: {learning_rate}")
        logging.info(f"âš¡ Warmup steps: {warmup_steps}")
        logging.info(f"ğŸ“ˆ Safe evaluator enabled: {evaluator is not None}")
        
        if evaluator is not None:
            self.sentence_transformer.fit(
                train_objectives=[(train_dataloader, train_loss)],
                epochs=epochs,
                warmup_steps=warmup_steps,
                evaluator=evaluator,
                evaluation_steps=35,  # ë” ìì£¼ í‰ê°€ (ë§¤ ì—í¬í¬ë§ˆë‹¤)
                output_path=self.output_path,
                save_best_model=True,
                show_progress_bar=True,
                optimizer_params={'lr': learning_rate},
                use_amp=False,  # Mixed precision ë¹„í™œì„±í™”
                checkpoint_save_steps=None,  # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë¹„í™œì„±í™”
                checkpoint_save_total_limit=None
            )
        else:
            # Fallback: í‰ê°€ì ì—†ì´ í•™ìŠµ
            self.sentence_transformer.fit(
                train_objectives=[(train_dataloader, train_loss)],
                epochs=epochs,
                warmup_steps=warmup_steps,
                output_path=self.output_path,
                show_progress_bar=True,
                optimizer_params={'lr': learning_rate}
            )
        
        logging.info(f"âœ… Training completed. Saved to: {self.output_path}")
    
    def load_trained_model(self, model_path: str = None):
        """í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ"""
        if model_path is None:
            model_path = self.output_path
        
        logging.info(f"Loading trained model from: {model_path}")
        self.sentence_transformer = SentenceTransformer(model_path)
    
    def evaluate_model_performance(self, test_df: pd.DataFrame) -> Dict:
        """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
        if self.sentence_transformer is None:
            raise ValueError("Model not loaded. Please train or load a model first.")
        
        # ì„ë² ë”© ìƒì„±
        queries = test_df['query'].tolist()
        values = test_df['value'].tolist()
        labels = test_df['label'].tolist()
        
        logging.info("ğŸ” Generating embeddings for evaluation...")
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        query_embeddings = self.sentence_transformer.encode(queries, show_progress_bar=True, device=device)
        value_embeddings = self.sentence_transformer.encode(values, show_progress_bar=True, device=device)
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []
        for q_emb, v_emb in zip(query_embeddings, value_embeddings):
            sim = cosine_similarity([q_emb], [v_emb])[0][0]
            similarities.append(sim)
        
        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        # ìœ ì‚¬ë„ë¥¼ ì´ì§„ ë¶„ë¥˜ë¡œ ë³€í™˜ (ì„ê³„ê°’ 0.5)
        predictions = [1 if sim > 0.5 else 0 for sim in similarities]
        
        metrics = {
            'accuracy': accuracy_score(labels, predictions),
            'precision': precision_score(labels, predictions, zero_division=0),
            'recall': recall_score(labels, predictions, zero_division=0),
            'f1_score': f1_score(labels, predictions, zero_division=0),
            'avg_similarity': np.mean(similarities),
            'std_similarity': np.std(similarities)
        }
        
        return metrics
    
    def recommend_products(self, 
                          user_query: str, 
                          product_values: List[str], 
                          top_k: int = 5) -> List[Tuple[int, float]]:
        """ì‚¬ìš©ì ì¿¼ë¦¬ì— ëŒ€í•´ ìƒí’ˆ ì¶”ì²œ"""
        if self.sentence_transformer is None:
            raise ValueError("Model not loaded. Please train or load a model first.")
        
        # ì„ë² ë”© ìƒì„±
        query_embedding = self.sentence_transformer.encode([user_query])
        product_embeddings = self.sentence_transformer.encode(product_values)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = cosine_similarity(query_embedding, product_embeddings)[0]
        
        # ìƒìœ„ kê°œ ì¶”ì²œ
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        recommendations = [(idx, similarities[idx]) for idx in top_indices]
        
        return recommendations

    def show_tokenization(self, df: pd.DataFrame, column: str = 'query', max_rows: int = 3):
        """íŠ¹ì • ì»¬ëŸ¼ì˜ í…ìŠ¤íŠ¸ë¥¼ tokenizerë¡œ í† í°í™”í•˜ì—¬ í™•ì¸í•˜ëŠ” í•¨ìˆ˜"""
        if self.tokenizer is None:
            raise ValueError("tokenizerê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_pretrained_model()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        sample_texts = df[column].head(max_rows).tolist()
        
        for i, text in enumerate(sample_texts):
            tokens = self.tokenizer.tokenize(text)
            token_ids = self.tokenizer.convert_tokens_to_ids(tokens)
            print(f"\n--- ìƒ˜í”Œ {i+1} ---")
            print(f"ì›ë¬¸: {text}")
            print(f"í† í°: {tokens}")
            print(f"í† í° ID: {token_ids}")


def split_data_by_date(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """ë‚ ì§œë³„ë¡œ ë°ì´í„°ë¥¼ ë¶„í• í•˜ëŠ” í•¨ìˆ˜"""
    if 'date' in df.columns:
        unique_dates = sorted(df['date'].unique())
        train_dates = unique_dates[:-1]  # ë§ˆì§€ë§‰ ë‚ ì§œ ì œì™¸í•˜ê³  í•™ìŠµ
        test_dates = [unique_dates[-1]]  # ë§ˆì§€ë§‰ ë‚ ì§œë¥¼ í…ŒìŠ¤íŠ¸
        
        train_df = df[df['date'].isin(train_dates)]
        test_df = df[df['date'].isin(test_dates)]
        
        logging.info(f"ğŸ“… Train dates: {train_dates}")
        logging.info(f"ğŸ“… Test dates: {test_dates}")
    else:
        # ë‚ ì§œ ì •ë³´ê°€ ì—†ìœ¼ë©´ ëœë¤ ë¶„í• 
        train_df, test_df = train_test_split(
            df, 
            test_size=TRAINING_CONFIG['test_size'], 
            random_state=TRAINING_CONFIG['random_state']
        )
    
    # í•™ìŠµ/ê²€ì¦ ë¶„í• 
    train_df, eval_df = train_test_split(
        train_df, 
        test_size=TRAINING_CONFIG['eval_size'], 
        random_state=TRAINING_CONFIG['random_state']
    )
    
    logging.info(f"ğŸ“ˆ Train samples: {len(train_df)}")
    logging.info(f"ğŸ“Š Eval samples: {len(eval_df)}")
    logging.info(f"ğŸ§ª Test samples: {len(test_df)}")
    
    return train_df, eval_df, test_df


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    from data_preprocessing import preprocess_insurance_data
    from data_converter import InsuranceDataConverter
    
    print("ğŸ§ª InsuranceEmbeddingTrainer í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    df = preprocess_insurance_data()
    sample_df = df.head(1000)  # ìƒ˜í”Œ 1000ê°œë§Œ í…ŒìŠ¤íŠ¸
    
    # ë°ì´í„° ë³€í™˜
    converter = InsuranceDataConverter()
    result_df = converter.convert_dataframe(sample_df)
    
    # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
    trainer = InsuranceEmbeddingTrainer()
    
    # ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
    try:
        trainer.load_pretrained_model()
        sample_sentences = result_df['query'].head(3).tolist()
        embeddings = trainer.get_embeddings_manual(sample_sentences)
        print(f"âœ… ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ì„ë² ë”© í…ŒìŠ¤íŠ¸: {embeddings.shape}")
    except Exception as e:
        print(f"âš ï¸ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    print("ğŸ¯ InsuranceEmbeddingTrainer í…ŒìŠ¤íŠ¸ ì™„ë£Œ") 