{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "aggregated_df = pd.read_parquet(\"/workspace/recomAI/aggregated_df_250625_info.parquet\")\n",
    "\n",
    "# í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "fin_aggregated_df = aggregated_df[['GNDR_CD', 'INS_AGE', 'JOB_GRD_CD', 'INJR_GRD', 'DRV_USG_DIV_CD', 'CHN_DIV',\n",
    "                                   'SBCP_YYMM', 'UNT_PD_NM', 'SLZ_PREM', 'PY_INS_PRD_NAME', 'LWRT_TMN_RFD_TP_CD',\n",
    "                                   'PY_EXEM_TP_CD', 'HNDY_ISP_TP_NM', 'PLAN_NM', 'PD_COV_NM', 'SBC_AMT',\n",
    "                                   'cov_ì¹˜ë§¤', 'cov_ì‹¬ì¥ì§ˆí™˜', 'cov_í›„ìœ ì¥í•´', 'cov_ë‡Œí˜ˆê´€ì§ˆí™˜', 'cov_ì•”', 'cov_ì‚¬ë§',\n",
    "                                   'cov_ê¸°íƒ€', 'cov_ì…ì›ë¹„(ì¼ë‹¹)', 'cov_ìš´ì „ì', 'cov_ì¹˜ì•„,í™”ìƒ,ê³¨ì ˆ', 'cov_ìˆ˜ìˆ ë¹„',\n",
    "                                   'cov_ì˜ë£Œë¹„', 'cov_ë²•ë¥ ,ë°°ìƒì±…ì„']].copy()\n",
    "\n",
    "# ë³´í—˜ë£Œ êµ¬ê°„ ì„¤ì •\n",
    "bins = [-np.inf, 50000, 100000, 150000, 200000, np.inf]\n",
    "labels = ['5ë§Œì› ì´í•˜', '5~10ë§Œì› ì´í•˜', '10~15ë§Œì› ì´í•˜', '15~20ë§Œì› ì´í•˜', '20ë§Œì› ì´ˆê³¼']\n",
    "fin_aggregated_df['tar_prem'] = pd.cut(fin_aggregated_df['SLZ_PREM'], bins=bins, labels=labels)\n",
    "\n",
    "# í…Œë§ˆ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸\n",
    "theme_columns = ['cov_ì¹˜ë§¤', 'cov_ì‹¬ì¥ì§ˆí™˜', 'cov_í›„ìœ ì¥í•´', 'cov_ë‡Œí˜ˆê´€ì§ˆí™˜', 'cov_ì•”', 'cov_ì‚¬ë§',\n",
    "                 'cov_ì…ì›ë¹„(ì¼ë‹¹)', 'cov_ìš´ì „ì', 'cov_ì¹˜ì•„,í™”ìƒ,ê³¨ì ˆ',\n",
    "                 'cov_ìˆ˜ìˆ ë¹„', 'cov_ì˜ë£Œë¹„', 'cov_ë²•ë¥ ,ë°°ìƒì±…ì„']\n",
    "\n",
    "# tar_theme ë§Œë“¤ê¸°: ê°’ì´ 1ë³´ë‹¤ í° í…Œë§ˆëª…ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥\n",
    "def extract_themes(row):\n",
    "    themes = [col.replace('cov_', '') for col in theme_columns if row[col] >= 1]\n",
    "    return ', '.join(themes) if themes else 'ì—†ìŒ'\n",
    "\n",
    "fin_aggregated_df['tar_theme'] = fin_aggregated_df.apply(extract_themes, axis=1)\n",
    "fin_aggregated_df = fin_aggregated_df.drop(columns=theme_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìœ ì €ì •ë³´ - ì„±ë³„, ë‚˜ì´, ì§ì—…ê¸‰ìˆ˜, ìƒí•´ë“±ê¸‰, ìš´ì „ìš©ë„, ì±„ë„, í¬ë§ë³´í—˜ë£Œ, í¬ë§í…Œë§ˆ\n",
    "# 'GNDR_CD', 'INS_AGE', 'JOB_GRD_CD', 'INJR_GRD', 'DRV_USG_DIV_CD' 'CHN_DIV'\n",
    "\n",
    "# ì•„ì´í…œì •ë³´ - ì²­ì•½ë…„ì›”, ìƒí’ˆì´ë¦„, ë³´í—˜ë£Œ, ë‚©ë§Œê¸°, ì €ìœ¨í•´ì§€, ë‚©ì…ë©´ì œ, ê°„í¸ì‹¬ì‚¬ìœ í˜•, í”Œëœ, ë¬´í•´ì§€ìœ í˜•, ë‚©ì…ë©´ì œìœ í˜•, ë‹´ë³´ëª…, ê°€ì…ê¸ˆì•¡\n",
    "# 'SBCP_YYMM', 'UNT_PD_NM', 'SLZ_PREM', 'PY_INS_PRD_NAME', 'LWRT_TMN_RFD_TP_CD', 'PY_EXEM_TP_CD', 'HNDY_ISP_TP_NM', 'PLAN_NM', 'PD_COV_NM', 'SBC_AMT', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "model_path = './MiniLM-L12-v2/0_Transformer'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "\n",
    "sentences = [\"í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤.\"]\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    attention_mask = inputs[\"attention_mask\"].unsqueeze(-1)\n",
    "    pooled = (embeddings * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "\n",
    "print(\"âœ… ìˆ˜ë™ ì„ë² ë”©:\", pooled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "class InsuranceDataConverter:\n",
    "    def __init__(self):\n",
    "        # ì½”ë“œ ë§¤í•‘ ì‚¬ì „ë“¤\n",
    "        self.gender_map = {\n",
    "            '1': 'ë‚¨ì„±',\n",
    "            '2': 'ì—¬ì„±'\n",
    "        }\n",
    "        \n",
    "        self.job_grade_map = {\n",
    "            '1': '1ê¸‰ ì‚¬ë¬´ì§',\n",
    "            '2': '2ê¸‰ ì¼ë°˜ì§',\n",
    "            '3': '3ê¸‰ ìœ„í—˜ì§',\n",
    "        }\n",
    "        \n",
    "        self.injury_grade_map = {\n",
    "            '1': '1ê¸‰ ë§¤ìš°ë‚®ìŒ',\n",
    "            '2': '2ê¸‰ ë‚®ìŒ', \n",
    "            '3': '3ê¸‰ ë‚®ìŒ',\n",
    "            '4': '4ê¸‰ ë³´í†µ',\n",
    "            '5': '5ê¸‰ ë†’ìŒ',\n",
    "            '6': '6ê¸‰ ë†’ìŒ',\n",
    "            '7': '7ê¸‰ ë§¤ìš°ë†’ìŒ',\n",
    "            '8': '8ê¸‰ ë§¤ìš°ë†’ìŒ',\n",
    "            '9': '9ê¸‰ ê³ ìœ„í—˜',\n",
    "            '10': '10ê¸‰ ê³ ìœ„í—˜'\n",
    "\n",
    "        }\n",
    "        \n",
    "        self.payment_exemption_map = {\n",
    "            '00': 'ë‚©ì…ë©´ì œ ë¯¸ì ìš©í˜•',\n",
    "            '01': 'ë‚©ì…ë©´ì œ1í˜•',\n",
    "            '02': 'ë‚©ì…ë©´ì œ2í˜•',\n",
    "            '03': 'ë‚©ì…ë©´ì œ3í˜•',\n",
    "            '04': 'ë‚©ì…ë©´ì œ4í˜•',\n",
    "            '05': 'ë‚©ì…ë©´ì œ5í˜•',\n",
    "            '06': 'ë‚©ì…ë©´ì œ6í˜•'\n",
    "        }\n",
    "        \n",
    "        self.surrender_refund_map = {\n",
    "            '00': 'í‘œì¤€í˜•',\n",
    "            '01': 'í•´ì§€í™˜ê¸‰ê¸ˆë¯¸ì§€ê¸‰í˜•',\n",
    "            '02': 'í•´ì§€í™˜ê¸‰ê¸ˆë¯¸ì§€ê¸‰í˜•',\n",
    "            '03': 'í•´ì§€í™˜ê¸‰ê¸ˆë¯¸ì§€ê¸‰í˜•',\n",
    "            '04': 'í•´ì§€í™˜ê¸‰ê¸ˆ50%ì§€ê¸‰í˜•',\n",
    "            '05': 'í•´ì§€í™˜ê¸‰ê¸ˆë¯¸ì§€ê¸‰í˜•',\n",
    "            '06': 'í•´ì§€í™˜ê¸‰ê¸ˆë¯¸ì§€ê¸‰í˜•',\n",
    "            '07': 'í•´ì§€í™˜ê¸‰ê¸ˆì§€ê¸‰í˜•'\n",
    "        }\n",
    "        \n",
    "        self.health_declaration_map = {\n",
    "            '01': 'ì¼ë°˜ê³ ì§€í˜•',\n",
    "            '02': 'ê±´ê°•ê³ ì§€í˜• 6ë…„',\n",
    "            '03': 'ê±´ê°•ê³ ì§€í˜• 7ë…„',\n",
    "            '04': 'ê±´ê°•ê³ ì§€í˜• 8ë…„',\n",
    "            '05': 'ê±´ê°•ê³ ì§€í˜• 9ë…„',\n",
    "            '06': 'ê±´ê°•ê³ ì§€í˜• 10ë…„'\n",
    "        }\n",
    "\n",
    "    def decode_value(self, value, mapping_dict):\n",
    "        \"\"\"ì½”ë“œê°’ì„ ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\"\"\"\n",
    "        if pd.isna(value):\n",
    "            return \"ì •ë³´ì—†ìŒ\"\n",
    "        return mapping_dict.get(str(value), str(value))\n",
    "\n",
    "    def format_premium(self, premium):\n",
    "        \"\"\"ë³´í—˜ë£Œë¥¼ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ í¬ë§·\"\"\"\n",
    "        if pd.isna(premium):\n",
    "            return \"ë³´í—˜ë£Œ ì •ë³´ì—†ìŒ\"\n",
    "        \n",
    "        premium = int(premium)\n",
    "        return self.format_amount_korean(premium)\n",
    "    \n",
    "    def format_amount_korean(self, amount):\n",
    "        \"\"\"ê¸ˆì•¡ì„ í•œêµ­ì–´ ë‹¨ìœ„ë¡œ í¬ë§· (ì˜ˆ: 50000 -> 5ë§Œì›)\"\"\"\n",
    "        if amount == 0:\n",
    "            return \"0ì›\"\n",
    "        \n",
    "        # ì–µ ë‹¨ìœ„\n",
    "        if amount >= 100000000:\n",
    "            uk = amount // 100000000\n",
    "            remainder = amount % 100000000\n",
    "            if remainder == 0:\n",
    "                return f\"{uk}ì–µì›\"\n",
    "            elif remainder >= 10000000:  # ì²œë§Œ ë‹¨ìœ„\n",
    "                man = remainder // 10000000\n",
    "                return f\"{uk}ì–µ {man}ì²œë§Œì›\"\n",
    "            elif remainder >= 10000:  # ë§Œ ë‹¨ìœ„\n",
    "                man = remainder // 10000\n",
    "                return f\"{uk}ì–µ {man}ë§Œì›\"\n",
    "            else:\n",
    "                return f\"{uk}ì–µ {remainder}ì›\"\n",
    "        \n",
    "        # ë§Œ ë‹¨ìœ„ \n",
    "        elif amount >= 10000:\n",
    "            man = amount // 10000\n",
    "            remainder = amount % 10000\n",
    "            if remainder == 0:\n",
    "                return f\"{man}ë§Œì›\"\n",
    "            else:\n",
    "                return f\"{man}ë§Œ {remainder}ì›\"\n",
    "        \n",
    "        # ë§Œ ë¯¸ë§Œ\n",
    "        else:\n",
    "            return f\"{amount}ì›\"\n",
    "\n",
    "    import re\n",
    "\n",
    "    def preprocess_product_name(self, name: str) -> str:\n",
    "        \"\"\"\n",
    "        ë³´í—˜ ìƒí’ˆëª… ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "\n",
    "        Args:\n",
    "            name: ì›ë³¸ ìƒí’ˆëª…\n",
    "\n",
    "        Returns:\n",
    "            ì „ì²˜ë¦¬ëœ ìƒí’ˆëª…\n",
    "        \"\"\"\n",
    "        # 1. '(ë¬´)' ì œê±°\n",
    "        name = re.sub(r'\\(ë¬´\\)', '', name)\n",
    "\n",
    "        # 2. 'ë©”ë¦¬ì¸ ' ì œê±°\n",
    "        name = re.sub(r'\\bë©”ë¦¬ì¸ \\b', '', name)\n",
    "\n",
    "        # 3. '2504' ë“± ìˆ«ì ì—°ë„ ì œê±° (2504, 2505 ë“±)\n",
    "        name = re.sub(r'\\d{4}', '', name)\n",
    "\n",
    "        # 4. ê´„í˜¸ ë‚´ í…ìŠ¤íŠ¸ ì œê±° (ë‹¨, ê°±ì‹ í˜•/ì„¸ë§Œê¸°í˜•ì€ ë‚¨ê¹€)\n",
    "        # ì˜ˆ: (í†µí•©ê°„í¸ì‹¬ì‚¬í˜•) â†’ ì œê±°\n",
    "        name = re.sub(r'\\((?!.*ê°±ì‹ í˜•|ì„¸ë§Œê¸°í˜•).*?\\)', '', name)\n",
    "\n",
    "        # 5. ê´„í˜¸ ìì²´ ì œê±° (ë‚¨ì€ ê²½ìš°)\n",
    "        name = re.sub(r'[()]', '', name)\n",
    "\n",
    "        # 6. ê³µë°± ì •ë¦¬\n",
    "        name = re.sub(r'\\s+', ' ', name).strip()\n",
    "\n",
    "        return name\n",
    "\n",
    "\n",
    "    def format_coverage_and_amounts(self, coverage_str, amount_str):\n",
    "        \"\"\"ë‹´ë³´ëª…ê³¼ ê°€ì…ê¸ˆì•¡ì„ ë§¤ì¹­í•˜ì—¬ í¬ë§·\"\"\"\n",
    "        if pd.isna(coverage_str) or pd.isna(amount_str):\n",
    "            return \"ë‹´ë³´ ì •ë³´ì—†ìŒ\"\n",
    "        \n",
    "        # ë‹´ë³´ëª… íŒŒì‹± (! êµ¬ë¶„)\n",
    "        coverages = coverage_str.split('!')\n",
    "        coverages = [cov.strip() for cov in coverages if cov.strip()]\n",
    "        \n",
    "        # ê°€ì…ê¸ˆì•¡ íŒŒì‹± (, êµ¬ë¶„)\n",
    "        amounts = str(amount_str).split(',')\n",
    "        amounts = [amt.strip() for amt in amounts if amt.strip()]\n",
    "        \n",
    "        # ë‹´ë³´ì™€ ê¸ˆì•¡ ë§¤ì¹­\n",
    "        coverage_list = []\n",
    "        for i, coverage in enumerate(coverages):\n",
    "            if i < len(amounts):\n",
    "                amount = amounts[i]\n",
    "                # ê¸ˆì•¡ í¬ë§·íŒ… (í•œêµ­ì–´ ë‹¨ìœ„ë¡œ)\n",
    "                try:\n",
    "                    amount_int = int(amount)\n",
    "                    formatted_amount = self.format_amount_korean(amount_int)\n",
    "                except:\n",
    "                    formatted_amount = amount\n",
    "                \n",
    "                # ë‹´ë³´ëª… ì •ë¦¬ (ë¶ˆí•„ìš”í•œ ê¸°í˜¸ ì œê±°)\n",
    "                clean_coverage = re.sub(r'[ê°±ì‹ í˜•|!\\[\\]()]', '', coverage).strip()\n",
    "                coverage_list.append(f\"{clean_coverage} {formatted_amount}\")\n",
    "            else:\n",
    "                clean_coverage = re.sub(r'[ê°±ì‹ í˜•|!\\[\\]()]', '', coverage).strip()\n",
    "                coverage_list.append(clean_coverage)\n",
    "        \n",
    "        return \", \".join(coverage_list[:3]) + (\"...\" if len(coverage_list) > 3 else \"\")\n",
    "\n",
    "    def format_target_theme(self, theme_str):\n",
    "        \"\"\"íƒ€ê²Ÿ í…Œë§ˆë¥¼ ì½ê¸° ì‰½ê²Œ í¬ë§·\"\"\"\n",
    "        if pd.isna(theme_str):\n",
    "            return \"íŠ¹ë³„í•œ ê´€ì‹¬ì‚¬í•­ ì—†ìŒ\"\n",
    "        \n",
    "        themes = str(theme_str).split(',')\n",
    "        themes = [theme.strip() for theme in themes if theme.strip()]\n",
    "        \n",
    "        theme_map = {\n",
    "            'ì‚¬ë§': 'ì‚¬ë§',\n",
    "            'ì•”': 'ì•”',\n",
    "            'ì¹˜ë§¤': 'ì¹˜ë§¤', \n",
    "            'ë‡Œì§ˆí™˜': 'ë‡Œí˜ˆê´€',\n",
    "            'ì‹¬ì¥ì§ˆí™˜': 'ì‹¬ì¥ì§ˆí™˜',\n",
    "            'ìˆ˜ìˆ ë¹„': 'ìˆ˜ìˆ ë¹„',\n",
    "            'ê°„ë³‘': 'ê°„ë³‘',\n",
    "            'ì¹˜ì•„': 'ì¹˜ì•„',\n",
    "            'í™”ìƒ': 'í™”ìƒ',\n",
    "            'ê³¨ì ˆ': 'ê³¨ì ˆ'\n",
    "        }\n",
    "        \n",
    "        formatted_themes = []\n",
    "        for theme in themes[:4]:  # ìµœëŒ€ 4ê°œë§Œ\n",
    "            formatted_themes.append(theme_map.get(theme, theme))\n",
    "        \n",
    "        return \", \".join(formatted_themes)\n",
    "\n",
    "    def convert_to_query_value_pair(self, row):\n",
    "        \"\"\"ë‹¨ì¼ í–‰ì„ Query-Value pairë¡œ ë³€í™˜\"\"\"\n",
    "        \n",
    "        # === QUERY: ì‚¬ìš©ì ì •ë³´ (ê³ ê° í”„ë¡œí•„) ===\n",
    "        gender = self.decode_value(row['GNDR_CD'], self.gender_map)\n",
    "        age = f\"{row['INS_AGE']}ì„¸\" if not pd.isna(row['INS_AGE']) else \"ì—°ë ¹ ì •ë³´ì—†ìŒ\"\n",
    "        job_grade = self.decode_value(row['JOB_GRD_CD'], self.job_grade_map)\n",
    "        injury_grade = self.decode_value(row['INJR_GRD'], self.injury_grade_map)\n",
    "        # driving_usage = self.decode_value(row['DRV_USG_DIV_CD'], self.driving_usage_map)\n",
    "        # channel = self.decode_value(row['CHN_DIV'], self.channel_map)\n",
    "        target_premium = row.get('tar_prem', 'í¬ë§ë³´í—˜ë£Œ ì •ë³´ì—†ìŒ')\n",
    "        target_theme = self.format_target_theme(row.get('tar_theme', ''))\n",
    "        \n",
    "        query = (\n",
    "            f\"{age} {gender} ê³ ê°ìœ¼ë¡œ ì§ì—…ë“±ê¸‰ {job_grade}, ìƒí•´ë“±ê¸‰ {injury_grade}ì— í•´ë‹¹í•©ë‹ˆë‹¤. \"\n",
    "            f\"í¬ë§í•˜ëŠ” ë³´í—˜ë£ŒëŠ” {target_premium}ì´ê³  {target_theme} í…Œë§ˆì— íŠ¹ë³„í•œ ê´€ì‹¬ì´ ìˆìŠµë‹ˆë‹¤.\"\n",
    "        )\n",
    "        \n",
    "        # === VALUE: ë³´í—˜ìƒí’ˆ ì •ë³´ (ì¦ê¶Œ ì •ë³´) ===\n",
    "        product_name = row.get('UNT_PD_NM', 'ìƒí’ˆëª… ì •ë³´ì—†ìŒ')\n",
    "        product_name = self.preprocess_product_name(product_name)\n",
    "        premium = self.format_premium(row.get('SLZ_PREM'))\n",
    "        payment_period = row.get('PY_INS_PRD_NAME', 'ë‚©ì…ê¸°ê°„ ì •ë³´ì—†ìŒ')\n",
    "        surrender_type = self.decode_value(row.get('LWRT_TMN_RFD_TP_CD'), self.surrender_refund_map)\n",
    "        payment_exemption = self.decode_value(row.get('PY_EXEM_TP_CD'), self.payment_exemption_map)\n",
    "        simple_review = row.get('HNDY_ISP_TP_NM', 'ì‹¬ì‚¬ìœ í˜• ì •ë³´ì—†ìŒ')\n",
    "        plan_name = row.get('PLAN_NM', '')\n",
    "        \n",
    "        # ë‹´ë³´ ì •ë³´\n",
    "        coverage_info = self.format_coverage_and_amounts(\n",
    "            row.get('PD_COV_NM'), \n",
    "            row.get('SBC_AMT')\n",
    "        )\n",
    "        \n",
    "        value = (\n",
    "            f\"{product_name} ìƒí’ˆìœ¼ë¡œ ì›” ë³´í—˜ë£Œ {premium}ì…ë‹ˆë‹¤. \"\n",
    "            f\"ë‚©ì…ì¡°ê±´ì€ {payment_period}ì´ë©° {surrender_type} ë°©ì‹ì„ ì ìš©í•©ë‹ˆë‹¤. \"\n",
    "            f\"{payment_exemption} ì¡°ê±´ì´ í¬í•¨ë˜ê³  {simple_review}ìœ¼ë¡œ ê°„í¸í•˜ê²Œ ê°€ì… ê°€ëŠ¥í•©ë‹ˆë‹¤.\"\n",
    "        )\n",
    "        \n",
    "        if plan_name and str(plan_name) != 'None' and str(plan_name).strip():\n",
    "            value += f\" {plan_name} í”Œëœì´ ì ìš©ë©ë‹ˆë‹¤.\"\n",
    "        \n",
    "        value += f\" ì£¼ìš” ë³´ì¥ë‚´ìš©: {coverage_info}\"\n",
    "        \n",
    "        return {\n",
    "            'date' : row.get('SBCP_YYMM'),\n",
    "            'query': query.strip(),\n",
    "            'value': value.strip(),\n",
    "            'label': 1\n",
    "        }\n",
    "\n",
    "    def convert_dataframe(self, df):\n",
    "        \"\"\"ì „ì²´ ë°ì´í„°í”„ë ˆì„ì„ Query-Value pairë¡œ ë³€í™˜\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            converted = self.convert_to_query_value_pair(row)\n",
    "            converted['original_index'] = idx\n",
    "            results.append(converted)\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "if __name__ == \"__main__\":\n",
    "    df = fin_aggregated_df #.head(1000)\n",
    "    \n",
    "    # ë³€í™˜ê¸° ì´ˆê¸°í™” ë° ì‹¤í–‰\n",
    "    converter = InsuranceDataConverter()\n",
    "    result_df = converter.convert_dataframe(df)\n",
    "\n",
    "    # Sentence Transformerìš© ë°ì´í„° ì¤€ë¹„\n",
    "    print(\"ğŸ¤– Sentence Transformer í•™ìŠµìš© ë°ì´í„°:\")\n",
    "    print(\"Queries:\", result_df['query'].tolist()[:3])\n",
    "    print(\"Values:\", result_df['value'].tolist()[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tokenization(trainer: InsuranceEmbeddingTrainer, df: pd.DataFrame, column: str = 'query', max_rows: int = 3):\n",
    "    \"\"\"\n",
    "    íŠ¹ì • ì»¬ëŸ¼ì˜ í…ìŠ¤íŠ¸ë¥¼ tokenizerë¡œ í† í°í™”í•˜ì—¬ í™•ì¸í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        trainer: InsuranceEmbeddingTrainer ì¸ìŠ¤í„´ìŠ¤ (tokenizer í¬í•¨)\n",
    "        df: result_df í˜•íƒœì˜ ë°ì´í„°í”„ë ˆì„\n",
    "        column: í† í°í™”í•  ì»¬ëŸ¼ëª… ('query' ë˜ëŠ” 'value')\n",
    "        max_rows: ì¶œë ¥í•  ìƒ˜í”Œ ìˆ˜\n",
    "    \"\"\"\n",
    "    if trainer.tokenizer is None:\n",
    "        raise ValueError(\"tokenizerê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_pretrained_model()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.\")\n",
    "    \n",
    "    sample_texts = df[column].head(max_rows).tolist()\n",
    "    \n",
    "    for i, text in enumerate(sample_texts):\n",
    "        tokens = trainer.tokenizer.tokenize(text)\n",
    "        token_ids = trainer.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        print(f\"\\n--- ìƒ˜í”Œ {i+1} ---\")\n",
    "        print(f\"ì›ë¬¸: {text}\")\n",
    "        print(f\"í† í°: {tokens}\")\n",
    "        print(f\"í† í° ID: {token_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ë¡œë”©\n",
    "trainer = InsuranceEmbeddingTrainer(\n",
    "    model_path='./MiniLM-L12-v2/0_Transformer'\n",
    ")\n",
    "trainer.load_pretrained_model()\n",
    "\n",
    "# result_dfì˜ query ì»¬ëŸ¼ í† í°í™” ë³´ê¸°\n",
    "show_tokenization(trainer, result_df, column='query')\n",
    "\n",
    "# value ì»¬ëŸ¼ë„ ë³´ê³  ì‹¶ë‹¤ë©´\n",
    "show_tokenization(trainer, result_df, column='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import logging\n",
    "import random\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# ë¡œê¹… ì„¤ì •\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "class InsuranceEmbeddingTrainer:\n",
    "    def __init__(self, \n",
    "                 model_path: str = './MiniLM-L12-v2/0_Transformer',\n",
    "                 output_path: str = './trained_insurance_model_retoken'):\n",
    "        \"\"\"\n",
    "        ë³´í—˜ ì¶”ì²œì„ ìœ„í•œ Embedding í•™ìŠµê¸°\n",
    "        \n",
    "        Args:\n",
    "            model_path: ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ\n",
    "            output_path: í•™ìŠµëœ ëª¨ë¸ ì €ì¥ ê²½ë¡œ\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.output_path = output_path\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.sentence_transformer = None\n",
    "        \n",
    "    def load_pretrained_model(self):\n",
    "        \"\"\"ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "        logging.info(f\"Loading pretrained model from: {self.model_path}\")\n",
    "        \n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "            self.model = AutoModel.from_pretrained(self.model_path)\n",
    "            logging.info(\"âœ… Pretrained model loaded successfully\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"âŒ Error loading pretrained model: {e}\")\n",
    "            # Fallback to online model\n",
    "            logging.info(\"Falling back to online model...\")\n",
    "            self.sentence_transformer = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    \n",
    "    def get_embeddings_manual(self, sentences: List[str]) -> torch.Tensor:\n",
    "        \"\"\"ìˆ˜ë™ìœ¼ë¡œ ì„ë² ë”© ì¶”ì¶œ (ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©)\"\"\"\n",
    "        if self.tokenizer is None or self.model is None:\n",
    "            raise ValueError(\"Pretrained model not loaded. Call load_pretrained_model() first.\")\n",
    "        \n",
    "        inputs = self.tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state\n",
    "            attention_mask = inputs[\"attention_mask\"].unsqueeze(-1)\n",
    "            # Mean pooling\n",
    "            pooled = (embeddings * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "        \n",
    "        return pooled\n",
    "    \n",
    "    def prepare_sentence_transformer(self):\n",
    "        \"\"\"Sentence Transformer ëª¨ë¸ ì¤€ë¹„\"\"\"\n",
    "        if self.sentence_transformer is None:\n",
    "            # ë¡œì»¬ ëª¨ë¸ì„ Sentence Transformerë¡œ ë³€í™˜\n",
    "            logging.info(\"Converting pretrained model to Sentence Transformer...\")\n",
    "            self.sentence_transformer = SentenceTransformer(self.model_path).to('cuda')\n",
    "          \n",
    "    \n",
    "    def prepare_training_data(self, df: pd.DataFrame) -> List[InputExample]:\n",
    "        \"\"\"\n",
    "        ì˜¬ë¦¬ë¸Œì˜ ë°©ì‹: Positive pairsë§Œ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ë°ì´í„° ì¤€ë¹„\n",
    "        \n",
    "        Args:\n",
    "            df: í•™ìŠµ ë°ì´í„°í”„ë ˆì„ (query, value, label ì»¬ëŸ¼ í•„ìš”)\n",
    "            \n",
    "        Returns:\n",
    "            InputExample ë¦¬ìŠ¤íŠ¸ (positive pairsë§Œ)\n",
    "        \"\"\"\n",
    "        examples = []\n",
    "        \n",
    "        logging.info(f\"Preparing training data with {len(df)} samples\")\n",
    "        \n",
    "        # Label=1ì¸ positive pairsë§Œ ì‚¬ìš©\n",
    "        positive_df = df[df['label'] == 1]\n",
    "        \n",
    "        for _, row in positive_df.iterrows():\n",
    "            # Label ì—†ì´ positive pairë§Œ ìƒì„±\n",
    "            # MultipleNegativesRankingLossê°€ ë°°ì¹˜ ë‚´ì—ì„œ ìë™ìœ¼ë¡œ negative sampling ìˆ˜í–‰\n",
    "            examples.append(InputExample(texts=[row['query'], row['value']]))\n",
    "        \n",
    "        logging.info(f\"Created {len(examples)} positive pairs\")\n",
    "        logging.info(\"Negative pairs will be automatically generated in-batch by MultipleNegativesRankingLoss\")\n",
    "        \n",
    "        # ë°ì´í„° ì…”í”Œ\n",
    "        random.shuffle(examples)\n",
    "        return examples\n",
    "    \n",
    "  \n",
    "    def create_simple_evaluator(self, eval_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        ê°„ë‹¨í•œ í‰ê°€ì ìƒì„± (ìƒê´€ê³„ìˆ˜ ëŒ€ì‹  ì •í™•ë„ ê¸°ë°˜)\n",
    "        \"\"\"\n",
    "        from sentence_transformers.evaluation import BinaryClassificationEvaluator\n",
    "        \n",
    "        # í‰ê°€ ë°ì´í„°ë¥¼ binary classification í˜•íƒœë¡œ ë³€í™˜\n",
    "        sentences1 = []\n",
    "        sentences2 = []\n",
    "        scores = []\n",
    "        \n",
    "        queries = eval_df['query'].tolist()\n",
    "        values = eval_df['value'].tolist()\n",
    "        \n",
    "        # Positive pairs (ì‹¤ì œ ë§¤ì¹­)\n",
    "        for query, value in zip(queries, values):\n",
    "            sentences1.append(query)\n",
    "            sentences2.append(value)\n",
    "            scores.append(1)  # positive\n",
    "        \n",
    "        # Negative pairs (ëœë¤ ë§¤ì¹­)\n",
    "        for i in range(min(len(queries), 20)):  # ìµœëŒ€ 20ê°œ negative\n",
    "            neg_idx = (i + len(queries) // 2) % len(values)  # ë‹¤ë¥¸ ì¸ë±ìŠ¤ ì„ íƒ\n",
    "            sentences1.append(queries[i])\n",
    "            sentences2.append(values[neg_idx])\n",
    "            scores.append(0)  # negative\n",
    "        \n",
    "        logging.info(f\"Created binary evaluator with {len(sentences1)} pairs\")\n",
    "        logging.info(f\"Positive: {scores.count(1)}, Negative: {scores.count(0)}\")\n",
    "        \n",
    "        return BinaryClassificationEvaluator(\n",
    "            sentences1=sentences1,\n",
    "            sentences2=sentences2,\n",
    "            labels=scores,\n",
    "            name=\"insurance_binary_eval\"\n",
    "        )\n",
    "    \n",
    "    def train_model_with_safe_evaluator(self, \n",
    "                                       train_df: pd.DataFrame,\n",
    "                                       eval_df: pd.DataFrame = None,\n",
    "                                       epochs: int = 4,\n",
    "                                       batch_size: int = 16,\n",
    "                                       learning_rate: float = 2e-5,\n",
    "                                       warmup_steps: int = None):\n",
    "        \"\"\"\n",
    "        ì•ˆì „í•œ í‰ê°€ìë¥¼ ì‚¬ìš©í•œ í•™ìŠµ\n",
    "        \"\"\"\n",
    "        # Sentence Transformer ì¤€ë¹„\n",
    "        self.prepare_sentence_transformer()\n",
    "        \n",
    "        # í•™ìŠµ ë°ì´í„° ì¤€ë¹„ (positive pairsë§Œ)\n",
    "        train_examples = self.prepare_training_data(train_df)\n",
    "        train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)\n",
    "        \n",
    "        # Loss function ì„¤ì • (MultipleNegativesRankingLoss)\n",
    "        train_loss = losses.MultipleNegativesRankingLoss(model=self.sentence_transformer)\n",
    "        \n",
    "        # Warmup steps ê³„ì‚°\n",
    "        if warmup_steps is None:\n",
    "            warmup_steps = int(len(train_dataloader) * epochs * 0.1)\n",
    "        \n",
    "        # ì•ˆì „í•œ í‰ê°€ì ìƒì„±\n",
    "        evaluator = None\n",
    "        if eval_df is not None and len(eval_df) >= 5:\n",
    "            try:\n",
    "                evaluator = self.create_simple_evaluator(eval_df)\n",
    "                logging.info(\"âœ… Safe binary evaluator created\")\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"âš ï¸ Failed to create safe evaluator: {e}\")\n",
    "                evaluator = None\n",
    "        \n",
    "        # ëª¨ë¸ í•™ìŠµ\n",
    "        logging.info(\"ğŸš€ Starting training with safe evaluator...\")\n",
    "        logging.info(f\"ğŸ“Š Training examples: {len(train_examples)}\")\n",
    "        logging.info(f\"ğŸ“¦ Batch size: {batch_size}\")\n",
    "        logging.info(f\"ğŸ”„ Epochs: {epochs}\")\n",
    "        logging.info(f\"ğŸ”¥ Learning rate: {learning_rate}\")\n",
    "        logging.info(f\"âš¡ Warmup steps: {warmup_steps}\")\n",
    "        logging.info(f\"ğŸ“ˆ Safe evaluator enabled: {evaluator is not None}\")\n",
    "        \n",
    "        if evaluator is not None:\n",
    "            self.sentence_transformer.fit(\n",
    "                train_objectives=[(train_dataloader, train_loss)],\n",
    "                epochs=epochs,\n",
    "                warmup_steps=warmup_steps,\n",
    "                evaluator=evaluator,\n",
    "                evaluation_steps=35,  # ë” ìì£¼ í‰ê°€ (ë§¤ ì—í¬í¬ë§ˆë‹¤)\n",
    "                output_path=self.output_path,\n",
    "                save_best_model=True,\n",
    "                show_progress_bar=True,\n",
    "                optimizer_params={'lr': learning_rate},\n",
    "                use_amp=False,  # Mixed precision ë¹„í™œì„±í™”\n",
    "                checkpoint_save_steps=None,  # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë¹„í™œì„±í™”\n",
    "                checkpoint_save_total_limit=None\n",
    "            )\n",
    "        else:\n",
    "            # Fallback: í‰ê°€ì ì—†ì´ í•™ìŠµ\n",
    "            self.sentence_transformer.fit(\n",
    "                train_objectives=[(train_dataloader, train_loss)],\n",
    "                epochs=epochs,\n",
    "                warmup_steps=warmup_steps,\n",
    "                output_path=self.output_path,\n",
    "                show_progress_bar=True,\n",
    "                optimizer_params={'lr': learning_rate}\n",
    "            )\n",
    "        \n",
    "        logging.info(f\"âœ… Training completed. Saved to: {self.output_path}\")\n",
    "    \n",
    "    def load_trained_model(self, model_path: str = None):\n",
    "        \"\"\"í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "        if model_path is None:\n",
    "            model_path = self.output_path\n",
    "        \n",
    "        logging.info(f\"Loading trained model from: {model_path}\")\n",
    "        self.sentence_transformer = SentenceTransformer(model_path)\n",
    "    \n",
    "    def evaluate_model_performance(self, test_df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\"\"\"\n",
    "        if self.sentence_transformer is None:\n",
    "            raise ValueError(\"Model not loaded. Please train or load a model first.\")\n",
    "        \n",
    "        # ì„ë² ë”© ìƒì„±\n",
    "        queries = test_df['query'].tolist()\n",
    "        values = test_df['value'].tolist()\n",
    "        labels = test_df['label'].tolist()\n",
    "        \n",
    "        logging.info(\"ğŸ” Generating embeddings for evaluation...\")\n",
    "        query_embeddings = self.sentence_transformer.encode(queries, show_progress_bar=True, device='cuda')\n",
    "        value_embeddings = self.sentence_transformer.encode(values, show_progress_bar=True, device='cuda')\n",
    "        \n",
    "        # ìœ ì‚¬ë„ ê³„ì‚°\n",
    "        similarities = []\n",
    "        for q_emb, v_emb in zip(query_embeddings, value_embeddings):\n",
    "            sim = cosine_similarity([q_emb], [v_emb])[0][0]\n",
    "            similarities.append(sim)\n",
    "        \n",
    "        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°\n",
    "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "        \n",
    "        # ìœ ì‚¬ë„ë¥¼ ì´ì§„ ë¶„ë¥˜ë¡œ ë³€í™˜ (ì„ê³„ê°’ 0.5)\n",
    "        predictions = [1 if sim > 0.5 else 0 for sim in similarities]\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(labels, predictions),\n",
    "            'precision': precision_score(labels, predictions, zero_division=0),\n",
    "            'recall': recall_score(labels, predictions, zero_division=0),\n",
    "            'f1_score': f1_score(labels, predictions, zero_division=0),\n",
    "            'avg_similarity': np.mean(similarities),\n",
    "            'std_similarity': np.std(similarities)\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def recommend_products(self, \n",
    "                          user_query: str, \n",
    "                          product_values: List[str], \n",
    "                          top_k: int = 5) -> List[Tuple[int, float]]:\n",
    "        \"\"\"ì‚¬ìš©ì ì¿¼ë¦¬ì— ëŒ€í•´ ìƒí’ˆ ì¶”ì²œ\"\"\"\n",
    "        if self.sentence_transformer is None:\n",
    "            raise ValueError(\"Model not loaded. Please train or load a model first.\")\n",
    "        \n",
    "        # ì„ë² ë”© ìƒì„±\n",
    "        query_embedding = self.sentence_transformer.encode([user_query])\n",
    "        product_embeddings = self.sentence_transformer.encode(product_values)\n",
    "        \n",
    "        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "        similarities = cosine_similarity(query_embedding, product_embeddings)[0]\n",
    "        \n",
    "        # ìƒìœ„ kê°œ ì¶”ì²œ\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        recommendations = [(idx, similarities[idx]) for idx in top_indices]\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "\n",
    "def main_training_pipeline(result_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    ë©”ì¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸\n",
    "    \n",
    "    Args:\n",
    "        result_df: InsuranceDataConverterë¡œ ë³€í™˜ëœ ë°ì´í„°í”„ë ˆì„\n",
    "    \"\"\"\n",
    "    \n",
    "    logging.info(\"ğŸ¯ ë³´í—˜ ì¶”ì²œ ëª¨ë¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹œì‘\")\n",
    "    \n",
    "    # 1. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„í• \n",
    "    logging.info(\"ğŸ“Š ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„í•  ì¤‘...\")\n",
    "    \n",
    "    # ë‚ ì§œë³„ë¡œ ë¶„í•  (ì‹œê°„ìˆœ ë¶„í• )\n",
    "    if 'date' in result_df.columns:\n",
    "        unique_dates = sorted(result_df['date'].unique())\n",
    "        train_dates = unique_dates[:-1]  # ë§ˆì§€ë§‰ ë‚ ì§œ ì œì™¸í•˜ê³  í•™ìŠµ\n",
    "        test_dates = [unique_dates[-1]]  # ë§ˆì§€ë§‰ ë‚ ì§œë¥¼ í…ŒìŠ¤íŠ¸\n",
    "        \n",
    "        train_df = result_df[result_df['date'].isin(train_dates)]\n",
    "        test_df = result_df[result_df['date'].isin(test_dates)]\n",
    "        \n",
    "        logging.info(f\"ğŸ“… Train dates: {train_dates}\")\n",
    "        logging.info(f\"ğŸ“… Test dates: {test_dates}\")\n",
    "    else:\n",
    "        # ë‚ ì§œ ì •ë³´ê°€ ì—†ìœ¼ë©´ ëœë¤ ë¶„í• \n",
    "        train_df, test_df = train_test_split(result_df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # í•™ìŠµ/ê²€ì¦ ë¶„í• \n",
    "    train_df, eval_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "    \n",
    "    logging.info(f\"ğŸ“ˆ Train samples: {len(train_df)}\")\n",
    "    logging.info(f\"ğŸ“Š Eval samples: {len(eval_df)}\")\n",
    "    logging.info(f\"ğŸ§ª Test samples: {len(test_df)}\")\n",
    "    \n",
    "    # 2. íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”\n",
    "    trainer = InsuranceEmbeddingTrainer(\n",
    "        model_path='./MiniLM-L12-v2/0_Transformer',\n",
    "        output_path='./trained_insurance_model_retoken'\n",
    "    )\n",
    "    \n",
    "    # 3. ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ (ì„ íƒì )\n",
    "    try:\n",
    "        trainer.load_pretrained_model()\n",
    "        \n",
    "        # ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ë¡œ ìƒ˜í”Œ ì„ë² ë”© í…ŒìŠ¤íŠ¸\n",
    "        sample_sentences = train_df['query'].head(3).tolist()\n",
    "        embeddings = trainer.get_embeddings_manual(sample_sentences)\n",
    "        logging.info(f\"âœ… ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ì„ë² ë”© í…ŒìŠ¤íŠ¸: {embeddings.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.warning(f\"âš ï¸ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "        logging.info(\"ğŸŒ ì˜¨ë¼ì¸ ëª¨ë¸ ì‚¬ìš©ìœ¼ë¡œ ì „í™˜\")\n",
    "    \n",
    "    # 4. ì•ˆì „í•œ í‰ê°€ìì™€ í•¨ê»˜ í•™ìŠµ\n",
    "    logging.info(\"ğŸš€ ì•ˆì „í•œ í‰ê°€ìì™€ í•¨ê»˜ í•™ìŠµ ì‹œì‘...\")\n",
    "    trainer.train_model_with_safe_evaluator(\n",
    "        train_df=train_df,\n",
    "        eval_df=eval_df,\n",
    "        epochs=4,\n",
    "        batch_size=16,\n",
    "        learning_rate=2e-5\n",
    "    )\n",
    "    \n",
    "    # 5. ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\n",
    "    logging.info(\"ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì¤‘...\")\n",
    "    metrics = trainer.evaluate_model_performance(test_df)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼\")\n",
    "    print(\"=\"*50)\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric:15}: {value:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 6. ì¶”ì²œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸\n",
    "    logging.info(\"ğŸ¯ ì¶”ì²œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì¤‘...\")\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìƒì„±\n",
    "    test_query = test_df['query'].iloc[0]\n",
    "    test_values = test_df['value'].head(10).tolist()\n",
    "    \n",
    "    recommendations = trainer.recommend_products(\n",
    "        user_query=test_query,\n",
    "        product_values=test_values,\n",
    "        top_k=5\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸ¯ ì¶”ì²œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ê²°ê³¼\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"ğŸ‘¤ ì‚¬ìš©ì ì¿¼ë¦¬: {test_query[:100]}...\")\n",
    "    print(\"\\nğŸ“‹ ì¶”ì²œ ìƒí’ˆ:\")\n",
    "    for i, (idx, score) in enumerate(recommendations):\n",
    "        print(f\"{i+1}. ìœ ì‚¬ë„: {score:.4f}\")\n",
    "        print(f\"   ìƒí’ˆ: {test_values[idx][:100]}...\")\n",
    "        print()\n",
    "    \n",
    "    # 7. ëª¨ë¸ ì €ì¥ í™•ì¸\n",
    "    if os.path.exists(trainer.output_path):\n",
    "        logging.info(f\"âœ… ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {trainer.output_path}\")\n",
    "    else:\n",
    "        logging.error(f\"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {trainer.output_path}\")\n",
    "    \n",
    "    return trainer, metrics\n",
    "\n",
    "\n",
    "# ì‹¤í–‰ ì˜ˆì‹œ\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    trainer, metrics = main_training_pipeline(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class AttentionAnalyzer:\n",
    "    \"\"\"ë³´í—˜ ì¶”ì²œ ëª¨ë¸ì˜ ì–´í…ì…˜ íŒ¨í„´ ë¶„ì„ ë„êµ¬\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str):\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\"\"\"\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "        self.model = AutoModel.from_pretrained(self.model_path, output_attentions=True)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        print(f\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_path}\")\n",
    "       \n",
    "    \n",
    "    def clean_token(self, token: str) -> str:\n",
    "        \"\"\"í† í° ì •ë¦¬ í•¨ìˆ˜\"\"\"\n",
    "        cleaned = token.replace('â–', '').replace('##', '')\n",
    "        if cleaned in ['[CLS]', '[SEP]', '[PAD]', '<s>', '</s>', '<pad>', '<unk>']:\n",
    "            return cleaned\n",
    "        if not cleaned.strip():\n",
    "            return '[SPACE]'\n",
    "        return cleaned\n",
    "    \n",
    "    def get_attention_weights(self, text: str) -> Tuple[List[str], torch.Tensor]:\n",
    "        \"\"\"í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì¶”ì¶œ\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            text, return_tensors=\"pt\", padding=True, \n",
    "            truncation=True, max_length=128\n",
    "        ).to(self.device)\n",
    "        \n",
    "        raw_tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "        print(f\"raw_tokens: {raw_tokens}\")\n",
    "        tokens = [self.clean_token(token) for token in raw_tokens]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            attention_weights = torch.stack(outputs.attentions)\n",
    "            attention_weights = attention_weights.squeeze(1)\n",
    "        \n",
    "        return tokens, attention_weights.cpu()\n",
    "    \n",
    "    def visualize_token_importance(self, text: str, layer_idx: int = -1, top_k: int = 10):\n",
    "        \"\"\"í† í° ì¤‘ìš”ë„ ë§‰ëŒ€ ê·¸ë˜í”„\"\"\"\n",
    "        tokens, attention_weights = self.get_attention_weights(text)\n",
    "        \n",
    "        if layer_idx == -1:\n",
    "            layer_idx = attention_weights.shape[0] - 1\n",
    "        attention = attention_weights[layer_idx].mean(dim=0)  # ëª¨ë“  í—¤ë“œ í‰ê· \n",
    "        \n",
    "        cls_attention = attention[0, :].numpy()  # CLS í† í°ì˜ ì–´í…ì…˜\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'token': tokens,\n",
    "            'importance': cls_attention,\n",
    "            'position': range(len(tokens))\n",
    "        })\n",
    "\n",
    "        df = df.sort_values('importance', ascending=False)\n",
    "        \n",
    "        # íŠ¹ìˆ˜ í† í° ì œì™¸\n",
    "        special_tokens = ['[CLS]', '[SEP]', '[PAD]', '<s>', '</s>', '<pad>', '[SPACE]']\n",
    "        df_filtered = df[~df['token'].isin(special_tokens)].head(top_k)\n",
    "        \n",
    "        # ì‹œê°í™”\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bars = plt.bar(range(len(df_filtered)), df_filtered['importance'], \n",
    "                       color=plt.cm.viridis(df_filtered['importance'] / df_filtered['importance'].max()))\n",
    "        \n",
    "        plt.xlabel('Tokens', fontsize=12)\n",
    "        plt.ylabel('Attention Weight', fontsize=12)\n",
    "        plt.title(f'Token Importance Analysis (Layer {layer_idx})\\nText: \"{text[:50]}...\"', \n",
    "                 fontsize=14, pad=20)\n",
    "        plt.xticks(range(len(df_filtered)), df_filtered['token'], rotation=45, ha='right')\n",
    "        \n",
    "        # ê°’ í‘œì‹œ\n",
    "        for i, (bar, val) in enumerate(zip(bars, df_filtered['importance'])):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "                    f'{val:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return df_filtered\n",
    "    \n",
    "    def visualize_attention_heatmap(self, text: str, layer_idx: int = -1, head_idx: int = 0):\n",
    "        \"\"\"ì–´í…ì…˜ íˆíŠ¸ë§µ ì‹œê°í™”\"\"\"\n",
    "        tokens, attention_weights = self.get_attention_weights(text)\n",
    "        \n",
    "        if layer_idx == -1:\n",
    "            layer_idx = attention_weights.shape[0] - 1\n",
    "        \n",
    "        attention = attention_weights[layer_idx, head_idx].numpy()\n",
    "        \n",
    "        # í† í° ê¸¸ì´ ì œí•œ\n",
    "        max_tokens = 100\n",
    "        if len(tokens) > max_tokens:\n",
    "            tokens = tokens[:max_tokens]\n",
    "            attention = attention[:max_tokens, :max_tokens]\n",
    "        \n",
    "        # í† í° ê¸¸ì´ ì œí•œ (í‘œì‹œìš©)\n",
    "        display_tokens = [token[:6] + '..' if len(token) > 8 else token for token in tokens]\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(\n",
    "            attention, xticklabels=display_tokens, yticklabels=display_tokens,\n",
    "            cmap='Blues', annot=False, cbar_kws={'label': 'Attention Weight'}\n",
    "        )\n",
    "        \n",
    "        plt.title(f'Attention Heatmap\\nLayer {layer_idx}, Head {head_idx}\\nText: \"{text[:50]}...\"', \n",
    "                 fontsize=14, pad=20)\n",
    "        plt.xlabel('Key Tokens', fontsize=12)\n",
    "        plt.ylabel('Query Tokens', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def analyze_test_samples(test_df: pd.DataFrame, \n",
    "                        model_path: str = \"./trained_insurance_model_retoken\",\n",
    "                        sample_size: int = 3):\n",
    "    \"\"\"\n",
    "    test_dfì—ì„œ ìƒ˜í”Œì„ ë½‘ì•„ì„œ ì–´í…ì…˜ ì‹œê°í™”\n",
    "    \n",
    "    Args:\n",
    "        test_df: í…ŒìŠ¤íŠ¸ ë°ì´í„°í”„ë ˆì„ (query, value ì»¬ëŸ¼ í•„ìš”)\n",
    "        model_path: ëª¨ë¸ ê²½ë¡œ\n",
    "        sample_size: ë¶„ì„í•  ìƒ˜í”Œ ê°œìˆ˜\n",
    "    \"\"\"\n",
    "    print(\"ğŸ” í…ŒìŠ¤íŠ¸ ë°ì´í„° ì–´í…ì…˜ ë¶„ì„ ì‹œì‘...\")\n",
    "    print(f\"ğŸ“Š test_df í¬ê¸°: {test_df.shape}\")\n",
    "    print(f\"ğŸ“‹ ì»¬ëŸ¼ëª…: {list(test_df.columns)}\")\n",
    "    \n",
    "    # ë¶„ì„ê¸° ìƒì„± ë° ëª¨ë¸ ë¡œë“œ\n",
    "    analyzer = AttentionAnalyzer(model_path)\n",
    "    analyzer.load_model()\n",
    "    \n",
    "    # ìƒ˜í”Œ ë°ì´í„° ì¶”ì¶œ\n",
    "    sample_data = test_df.sample(n=min(sample_size, len(test_df)), random_state=42)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š {len(sample_data)}ê°œ ìƒ˜í”Œ ë¶„ì„ ì‹œì‘\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, (idx, row) in enumerate(sample_data.iterrows()):\n",
    "        query = str(row['query'])\n",
    "        value = str(row['value'])\n",
    "        \n",
    "        print(f\"\\nğŸ” ìƒ˜í”Œ {i+1}/{len(sample_data)}\")\n",
    "        print(f\"ğŸ“ ì¿¼ë¦¬: {query}...\")\n",
    "        print(f\"ğŸ·ï¸ ìƒí’ˆ: {value}...\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # 1. ì¿¼ë¦¬ í† í° ì¤‘ìš”ë„ ë¶„ì„\n",
    "        print(\"ğŸ“Š ì¿¼ë¦¬ í† í° ì¤‘ìš”ë„:\")\n",
    "        query_df = analyzer.visualize_token_importance(query, top_k=8)\n",
    "        query_top_tokens = query_df['token'].head(3).tolist()\n",
    "        print(f\"ìƒìœ„ í† í°: {', '.join(query_top_tokens)}\")\n",
    "        \n",
    "        # 2. ìƒí’ˆ í† í° ì¤‘ìš”ë„ ë¶„ì„  \n",
    "        print(\"\\nğŸ“Š ìƒí’ˆ í† í° ì¤‘ìš”ë„:\")\n",
    "        value_df = analyzer.visualize_token_importance(value, top_k=8)\n",
    "        value_top_tokens = value_df['token'].head(3).tolist()\n",
    "        print(f\"ìƒìœ„ í† í°: {', '.join(value_top_tokens)}\")\n",
    "        \n",
    "        # 3. ì¿¼ë¦¬ ì–´í…ì…˜ íˆíŠ¸ë§µ\n",
    "        print(\"\\nğŸ“Š ì¿¼ë¦¬ ì–´í…ì…˜ íˆíŠ¸ë§µ:\")\n",
    "        analyzer.visualize_attention_heatmap(query, layer_idx=-1, head_idx=0)\n",
    "        \n",
    "        results.append({\n",
    "            'index': idx,\n",
    "            'query': query,\n",
    "            'value': value,\n",
    "            'query_top_tokens': query_top_tokens,\n",
    "            'value_top_tokens': value_top_tokens,\n",
    "            'query_analysis': query_df,\n",
    "            'value_analysis': value_df\n",
    "        })\n",
    "        \n",
    "        print(f\"âœ… ìƒ˜í”Œ {i+1} ë¶„ì„ ì™„ë£Œ\\n\")\n",
    "    \n",
    "    print(\"ğŸ‰ ëª¨ë“  ìƒ˜í”Œ ë¶„ì„ ì™„ë£Œ!\")\n",
    "    return analyzer, results\n",
    "\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "if __name__ == \"__main__\":\n",
    "    # test_dfê°€ ìˆë‹¤ê³  ê°€ì •í•˜ê³  ì‚¬ìš©\n",
    "    # ì›ë˜ í•™ìŠµ íŒŒì´í”„ë¼ì¸ê³¼ ì •í™•íˆ ë™ì¼\n",
    "    unique_dates = sorted(result_df['date'].unique())\n",
    "    train_dates = unique_dates[:-1]  # ë§ˆì§€ë§‰ ë‚ ì§œ ì œì™¸\n",
    "    test_dates = [unique_dates[-1]]  # ë§ˆì§€ë§‰ ë‚ ì§œë§Œ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "    train_df = result_df[result_df['date'].isin(train_dates)]\n",
    "    test_df = result_df[result_df['date'].isin(test_dates)]\\\n",
    "    \n",
    "    analyzer, results = analyze_test_samples(\n",
    "        test_df=test_df, \n",
    "        model_path=\"./trained_insurance_model_retoken\",\n",
    "        sample_size=3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
