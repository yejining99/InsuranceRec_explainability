{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "aggregated_df = pd.read_parquet(\"/workspace/recomAI/aggregated_df_250625_info.parquet\")\n",
    "\n",
    "# 필요한 컬럼만 선택\n",
    "fin_aggregated_df = aggregated_df[['GNDR_CD', 'INS_AGE', 'JOB_GRD_CD', 'INJR_GRD', 'DRV_USG_DIV_CD', 'CHN_DIV',\n",
    "                                   'SBCP_YYMM', 'UNT_PD_NM', 'SLZ_PREM', 'PY_INS_PRD_NAME', 'LWRT_TMN_RFD_TP_CD',\n",
    "                                   'PY_EXEM_TP_CD', 'HNDY_ISP_TP_NM', 'PLAN_NM', 'PD_COV_NM', 'SBC_AMT',\n",
    "                                   'cov_치매', 'cov_심장질환', 'cov_후유장해', 'cov_뇌혈관질환', 'cov_암', 'cov_사망',\n",
    "                                   'cov_기타', 'cov_입원비(일당)', 'cov_운전자', 'cov_치아,화상,골절', 'cov_수술비',\n",
    "                                   'cov_의료비', 'cov_법률,배상책임']].copy()\n",
    "\n",
    "# 보험료 구간 설정\n",
    "bins = [-np.inf, 50000, 100000, 150000, 200000, np.inf]\n",
    "labels = ['5만원 이하', '5~10만원 이하', '10~15만원 이하', '15~20만원 이하', '20만원 초과']\n",
    "fin_aggregated_df['tar_prem'] = pd.cut(fin_aggregated_df['SLZ_PREM'], bins=bins, labels=labels)\n",
    "\n",
    "# 테마 컬럼 리스트\n",
    "theme_columns = ['cov_치매', 'cov_심장질환', 'cov_후유장해', 'cov_뇌혈관질환', 'cov_암', 'cov_사망',\n",
    "                 'cov_입원비(일당)', 'cov_운전자', 'cov_치아,화상,골절',\n",
    "                 'cov_수술비', 'cov_의료비', 'cov_법률,배상책임']\n",
    "\n",
    "# tar_theme 만들기: 값이 1보다 큰 테마명을 리스트로 저장\n",
    "def extract_themes(row):\n",
    "    themes = [col.replace('cov_', '') for col in theme_columns if row[col] >= 1]\n",
    "    return ', '.join(themes) if themes else '없음'\n",
    "\n",
    "fin_aggregated_df['tar_theme'] = fin_aggregated_df.apply(extract_themes, axis=1)\n",
    "fin_aggregated_df = fin_aggregated_df.drop(columns=theme_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유저정보 - 성별, 나이, 직업급수, 상해등급, 운전용도, 채널, 희망보험료, 희망테마\n",
    "# 'GNDR_CD', 'INS_AGE', 'JOB_GRD_CD', 'INJR_GRD', 'DRV_USG_DIV_CD' 'CHN_DIV'\n",
    "\n",
    "# 아이템정보 - 청약년월, 상품이름, 보험료, 납만기, 저율해지, 납입면제, 간편심사유형, 플랜, 무해지유형, 납입면제유형, 담보명, 가입금액\n",
    "# 'SBCP_YYMM', 'UNT_PD_NM', 'SLZ_PREM', 'PY_INS_PRD_NAME', 'LWRT_TMN_RFD_TP_CD', 'PY_EXEM_TP_CD', 'HNDY_ISP_TP_NM', 'PLAN_NM', 'PD_COV_NM', 'SBC_AMT', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "model_path = './MiniLM-L12-v2/0_Transformer'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "\n",
    "sentences = [\"테스트 문장입니다.\"]\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    attention_mask = inputs[\"attention_mask\"].unsqueeze(-1)\n",
    "    pooled = (embeddings * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "\n",
    "print(\"✅ 수동 임베딩:\", pooled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "class InsuranceDataConverter:\n",
    "    def __init__(self):\n",
    "        # 코드 매핑 사전들\n",
    "        self.gender_map = {\n",
    "            '1': '남성',\n",
    "            '2': '여성'\n",
    "        }\n",
    "        \n",
    "        self.job_grade_map = {\n",
    "            '1': '1급 사무직',\n",
    "            '2': '2급 일반직',\n",
    "            '3': '3급 위험직',\n",
    "        }\n",
    "        \n",
    "        self.injury_grade_map = {\n",
    "            '1': '1급 매우낮음',\n",
    "            '2': '2급 낮음', \n",
    "            '3': '3급 낮음',\n",
    "            '4': '4급 보통',\n",
    "            '5': '5급 높음',\n",
    "            '6': '6급 높음',\n",
    "            '7': '7급 매우높음',\n",
    "            '8': '8급 매우높음',\n",
    "            '9': '9급 고위험',\n",
    "            '10': '10급 고위험'\n",
    "\n",
    "        }\n",
    "        \n",
    "        self.payment_exemption_map = {\n",
    "            '00': '납입면제 미적용형',\n",
    "            '01': '납입면제1형',\n",
    "            '02': '납입면제2형',\n",
    "            '03': '납입면제3형',\n",
    "            '04': '납입면제4형',\n",
    "            '05': '납입면제5형',\n",
    "            '06': '납입면제6형'\n",
    "        }\n",
    "        \n",
    "        self.surrender_refund_map = {\n",
    "            '00': '표준형',\n",
    "            '01': '해지환급금미지급형',\n",
    "            '02': '해지환급금미지급형',\n",
    "            '03': '해지환급금미지급형',\n",
    "            '04': '해지환급금50%지급형',\n",
    "            '05': '해지환급금미지급형',\n",
    "            '06': '해지환급금미지급형',\n",
    "            '07': '해지환급금지급형'\n",
    "        }\n",
    "        \n",
    "        self.health_declaration_map = {\n",
    "            '01': '일반고지형',\n",
    "            '02': '건강고지형 6년',\n",
    "            '03': '건강고지형 7년',\n",
    "            '04': '건강고지형 8년',\n",
    "            '05': '건강고지형 9년',\n",
    "            '06': '건강고지형 10년'\n",
    "        }\n",
    "\n",
    "    def decode_value(self, value, mapping_dict):\n",
    "        \"\"\"코드값을 의미있는 텍스트로 변환\"\"\"\n",
    "        if pd.isna(value):\n",
    "            return \"정보없음\"\n",
    "        return mapping_dict.get(str(value), str(value))\n",
    "\n",
    "    def format_premium(self, premium):\n",
    "        \"\"\"보험료를 읽기 쉬운 형태로 포맷\"\"\"\n",
    "        if pd.isna(premium):\n",
    "            return \"보험료 정보없음\"\n",
    "        \n",
    "        premium = int(premium)\n",
    "        return self.format_amount_korean(premium)\n",
    "    \n",
    "    def format_amount_korean(self, amount):\n",
    "        \"\"\"금액을 한국어 단위로 포맷 (예: 50000 -> 5만원)\"\"\"\n",
    "        if amount == 0:\n",
    "            return \"0원\"\n",
    "        \n",
    "        # 억 단위\n",
    "        if amount >= 100000000:\n",
    "            uk = amount // 100000000\n",
    "            remainder = amount % 100000000\n",
    "            if remainder == 0:\n",
    "                return f\"{uk}억원\"\n",
    "            elif remainder >= 10000000:  # 천만 단위\n",
    "                man = remainder // 10000000\n",
    "                return f\"{uk}억 {man}천만원\"\n",
    "            elif remainder >= 10000:  # 만 단위\n",
    "                man = remainder // 10000\n",
    "                return f\"{uk}억 {man}만원\"\n",
    "            else:\n",
    "                return f\"{uk}억 {remainder}원\"\n",
    "        \n",
    "        # 만 단위 \n",
    "        elif amount >= 10000:\n",
    "            man = amount // 10000\n",
    "            remainder = amount % 10000\n",
    "            if remainder == 0:\n",
    "                return f\"{man}만원\"\n",
    "            else:\n",
    "                return f\"{man}만 {remainder}원\"\n",
    "        \n",
    "        # 만 미만\n",
    "        else:\n",
    "            return f\"{amount}원\"\n",
    "\n",
    "    import re\n",
    "\n",
    "    def preprocess_product_name(self, name: str) -> str:\n",
    "        \"\"\"\n",
    "        보험 상품명 전처리 함수\n",
    "\n",
    "        Args:\n",
    "            name: 원본 상품명\n",
    "\n",
    "        Returns:\n",
    "            전처리된 상품명\n",
    "        \"\"\"\n",
    "        # 1. '(무)' 제거\n",
    "        name = re.sub(r'\\(무\\)', '', name)\n",
    "\n",
    "        # 2. '메리츠' 제거\n",
    "        name = re.sub(r'\\b메리츠\\b', '', name)\n",
    "\n",
    "        # 3. '2504' 등 숫자 연도 제거 (2504, 2505 등)\n",
    "        name = re.sub(r'\\d{4}', '', name)\n",
    "\n",
    "        # 4. 괄호 내 텍스트 제거 (단, 갱신형/세만기형은 남김)\n",
    "        # 예: (통합간편심사형) → 제거\n",
    "        name = re.sub(r'\\((?!.*갱신형|세만기형).*?\\)', '', name)\n",
    "\n",
    "        # 5. 괄호 자체 제거 (남은 경우)\n",
    "        name = re.sub(r'[()]', '', name)\n",
    "\n",
    "        # 6. 공백 정리\n",
    "        name = re.sub(r'\\s+', ' ', name).strip()\n",
    "\n",
    "        return name\n",
    "\n",
    "\n",
    "    def format_coverage_and_amounts(self, coverage_str, amount_str):\n",
    "        \"\"\"담보명과 가입금액을 매칭하여 포맷\"\"\"\n",
    "        if pd.isna(coverage_str) or pd.isna(amount_str):\n",
    "            return \"담보 정보없음\"\n",
    "        \n",
    "        # 담보명 파싱 (! 구분)\n",
    "        coverages = coverage_str.split('!')\n",
    "        coverages = [cov.strip() for cov in coverages if cov.strip()]\n",
    "        \n",
    "        # 가입금액 파싱 (, 구분)\n",
    "        amounts = str(amount_str).split(',')\n",
    "        amounts = [amt.strip() for amt in amounts if amt.strip()]\n",
    "        \n",
    "        # 담보와 금액 매칭\n",
    "        coverage_list = []\n",
    "        for i, coverage in enumerate(coverages):\n",
    "            if i < len(amounts):\n",
    "                amount = amounts[i]\n",
    "                # 금액 포맷팅 (한국어 단위로)\n",
    "                try:\n",
    "                    amount_int = int(amount)\n",
    "                    formatted_amount = self.format_amount_korean(amount_int)\n",
    "                except:\n",
    "                    formatted_amount = amount\n",
    "                \n",
    "                # 담보명 정리 (불필요한 기호 제거)\n",
    "                clean_coverage = re.sub(r'[갱신형|!\\[\\]()]', '', coverage).strip()\n",
    "                coverage_list.append(f\"{clean_coverage} {formatted_amount}\")\n",
    "            else:\n",
    "                clean_coverage = re.sub(r'[갱신형|!\\[\\]()]', '', coverage).strip()\n",
    "                coverage_list.append(clean_coverage)\n",
    "        \n",
    "        return \", \".join(coverage_list[:3]) + (\"...\" if len(coverage_list) > 3 else \"\")\n",
    "\n",
    "    def format_target_theme(self, theme_str):\n",
    "        \"\"\"타겟 테마를 읽기 쉽게 포맷\"\"\"\n",
    "        if pd.isna(theme_str):\n",
    "            return \"특별한 관심사항 없음\"\n",
    "        \n",
    "        themes = str(theme_str).split(',')\n",
    "        themes = [theme.strip() for theme in themes if theme.strip()]\n",
    "        \n",
    "        theme_map = {\n",
    "            '사망': '사망',\n",
    "            '암': '암',\n",
    "            '치매': '치매', \n",
    "            '뇌질환': '뇌혈관',\n",
    "            '심장질환': '심장질환',\n",
    "            '수술비': '수술비',\n",
    "            '간병': '간병',\n",
    "            '치아': '치아',\n",
    "            '화상': '화상',\n",
    "            '골절': '골절'\n",
    "        }\n",
    "        \n",
    "        formatted_themes = []\n",
    "        for theme in themes[:4]:  # 최대 4개만\n",
    "            formatted_themes.append(theme_map.get(theme, theme))\n",
    "        \n",
    "        return \", \".join(formatted_themes)\n",
    "\n",
    "    def convert_to_query_value_pair(self, row):\n",
    "        \"\"\"단일 행을 Query-Value pair로 변환\"\"\"\n",
    "        \n",
    "        # === QUERY: 사용자 정보 (고객 프로필) ===\n",
    "        gender = self.decode_value(row['GNDR_CD'], self.gender_map)\n",
    "        age = f\"{row['INS_AGE']}세\" if not pd.isna(row['INS_AGE']) else \"연령 정보없음\"\n",
    "        job_grade = self.decode_value(row['JOB_GRD_CD'], self.job_grade_map)\n",
    "        injury_grade = self.decode_value(row['INJR_GRD'], self.injury_grade_map)\n",
    "        # driving_usage = self.decode_value(row['DRV_USG_DIV_CD'], self.driving_usage_map)\n",
    "        # channel = self.decode_value(row['CHN_DIV'], self.channel_map)\n",
    "        target_premium = row.get('tar_prem', '희망보험료 정보없음')\n",
    "        target_theme = self.format_target_theme(row.get('tar_theme', ''))\n",
    "        \n",
    "        query = (\n",
    "            f\"{age} {gender} 고객으로 직업등급 {job_grade}, 상해등급 {injury_grade}에 해당합니다. \"\n",
    "            f\"희망하는 보험료는 {target_premium}이고 {target_theme} 테마에 특별한 관심이 있습니다.\"\n",
    "        )\n",
    "        \n",
    "        # === VALUE: 보험상품 정보 (증권 정보) ===\n",
    "        product_name = row.get('UNT_PD_NM', '상품명 정보없음')\n",
    "        product_name = self.preprocess_product_name(product_name)\n",
    "        premium = self.format_premium(row.get('SLZ_PREM'))\n",
    "        payment_period = row.get('PY_INS_PRD_NAME', '납입기간 정보없음')\n",
    "        surrender_type = self.decode_value(row.get('LWRT_TMN_RFD_TP_CD'), self.surrender_refund_map)\n",
    "        payment_exemption = self.decode_value(row.get('PY_EXEM_TP_CD'), self.payment_exemption_map)\n",
    "        simple_review = row.get('HNDY_ISP_TP_NM', '심사유형 정보없음')\n",
    "        plan_name = row.get('PLAN_NM', '')\n",
    "        \n",
    "        # 담보 정보\n",
    "        coverage_info = self.format_coverage_and_amounts(\n",
    "            row.get('PD_COV_NM'), \n",
    "            row.get('SBC_AMT')\n",
    "        )\n",
    "        \n",
    "        value = (\n",
    "            f\"{product_name} 상품으로 월 보험료 {premium}입니다. \"\n",
    "            f\"납입조건은 {payment_period}이며 {surrender_type} 방식을 적용합니다. \"\n",
    "            f\"{payment_exemption} 조건이 포함되고 {simple_review}으로 간편하게 가입 가능합니다.\"\n",
    "        )\n",
    "        \n",
    "        if plan_name and str(plan_name) != 'None' and str(plan_name).strip():\n",
    "            value += f\" {plan_name} 플랜이 적용됩니다.\"\n",
    "        \n",
    "        value += f\" 주요 보장내용: {coverage_info}\"\n",
    "        \n",
    "        return {\n",
    "            'date' : row.get('SBCP_YYMM'),\n",
    "            'query': query.strip(),\n",
    "            'value': value.strip(),\n",
    "            'label': 1\n",
    "        }\n",
    "\n",
    "    def convert_dataframe(self, df):\n",
    "        \"\"\"전체 데이터프레임을 Query-Value pair로 변환\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            converted = self.convert_to_query_value_pair(row)\n",
    "            converted['original_index'] = idx\n",
    "            results.append(converted)\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# 사용 예시\n",
    "if __name__ == \"__main__\":\n",
    "    df = fin_aggregated_df #.head(1000)\n",
    "    \n",
    "    # 변환기 초기화 및 실행\n",
    "    converter = InsuranceDataConverter()\n",
    "    result_df = converter.convert_dataframe(df)\n",
    "\n",
    "    # Sentence Transformer용 데이터 준비\n",
    "    print(\"🤖 Sentence Transformer 학습용 데이터:\")\n",
    "    print(\"Queries:\", result_df['query'].tolist()[:3])\n",
    "    print(\"Values:\", result_df['value'].tolist()[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tokenization(trainer: InsuranceEmbeddingTrainer, df: pd.DataFrame, column: str = 'query', max_rows: int = 3):\n",
    "    \"\"\"\n",
    "    특정 컬럼의 텍스트를 tokenizer로 토큰화하여 확인하는 함수\n",
    "    \n",
    "    Args:\n",
    "        trainer: InsuranceEmbeddingTrainer 인스턴스 (tokenizer 포함)\n",
    "        df: result_df 형태의 데이터프레임\n",
    "        column: 토큰화할 컬럼명 ('query' 또는 'value')\n",
    "        max_rows: 출력할 샘플 수\n",
    "    \"\"\"\n",
    "    if trainer.tokenizer is None:\n",
    "        raise ValueError(\"tokenizer가 초기화되지 않았습니다. load_pretrained_model()을 먼저 호출하세요.\")\n",
    "    \n",
    "    sample_texts = df[column].head(max_rows).tolist()\n",
    "    \n",
    "    for i, text in enumerate(sample_texts):\n",
    "        tokens = trainer.tokenizer.tokenize(text)\n",
    "        token_ids = trainer.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        print(f\"\\n--- 샘플 {i+1} ---\")\n",
    "        print(f\"원문: {text}\")\n",
    "        print(f\"토큰: {tokens}\")\n",
    "        print(f\"토큰 ID: {token_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로딩\n",
    "trainer = InsuranceEmbeddingTrainer(\n",
    "    model_path='./MiniLM-L12-v2/0_Transformer'\n",
    ")\n",
    "trainer.load_pretrained_model()\n",
    "\n",
    "# result_df의 query 컬럼 토큰화 보기\n",
    "show_tokenization(trainer, result_df, column='query')\n",
    "\n",
    "# value 컬럼도 보고 싶다면\n",
    "show_tokenization(trainer, result_df, column='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import logging\n",
    "import random\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# 로깅 설정\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "class InsuranceEmbeddingTrainer:\n",
    "    def __init__(self, \n",
    "                 model_path: str = './MiniLM-L12-v2/0_Transformer',\n",
    "                 output_path: str = './trained_insurance_model_retoken'):\n",
    "        \"\"\"\n",
    "        보험 추천을 위한 Embedding 학습기\n",
    "        \n",
    "        Args:\n",
    "            model_path: 사전 학습된 모델 경로\n",
    "            output_path: 학습된 모델 저장 경로\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.output_path = output_path\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.sentence_transformer = None\n",
    "        \n",
    "    def load_pretrained_model(self):\n",
    "        \"\"\"사전 학습된 모델 로드\"\"\"\n",
    "        logging.info(f\"Loading pretrained model from: {self.model_path}\")\n",
    "        \n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "            self.model = AutoModel.from_pretrained(self.model_path)\n",
    "            logging.info(\"✅ Pretrained model loaded successfully\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"❌ Error loading pretrained model: {e}\")\n",
    "            # Fallback to online model\n",
    "            logging.info(\"Falling back to online model...\")\n",
    "            self.sentence_transformer = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    \n",
    "    def get_embeddings_manual(self, sentences: List[str]) -> torch.Tensor:\n",
    "        \"\"\"수동으로 임베딩 추출 (사전 학습된 모델 사용)\"\"\"\n",
    "        if self.tokenizer is None or self.model is None:\n",
    "            raise ValueError(\"Pretrained model not loaded. Call load_pretrained_model() first.\")\n",
    "        \n",
    "        inputs = self.tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state\n",
    "            attention_mask = inputs[\"attention_mask\"].unsqueeze(-1)\n",
    "            # Mean pooling\n",
    "            pooled = (embeddings * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "        \n",
    "        return pooled\n",
    "    \n",
    "    def prepare_sentence_transformer(self):\n",
    "        \"\"\"Sentence Transformer 모델 준비\"\"\"\n",
    "        if self.sentence_transformer is None:\n",
    "            # 로컬 모델을 Sentence Transformer로 변환\n",
    "            logging.info(\"Converting pretrained model to Sentence Transformer...\")\n",
    "            self.sentence_transformer = SentenceTransformer(self.model_path).to('cuda')\n",
    "          \n",
    "    \n",
    "    def prepare_training_data(self, df: pd.DataFrame) -> List[InputExample]:\n",
    "        \"\"\"\n",
    "        올리브영 방식: Positive pairs만 사용하여 학습 데이터 준비\n",
    "        \n",
    "        Args:\n",
    "            df: 학습 데이터프레임 (query, value, label 컬럼 필요)\n",
    "            \n",
    "        Returns:\n",
    "            InputExample 리스트 (positive pairs만)\n",
    "        \"\"\"\n",
    "        examples = []\n",
    "        \n",
    "        logging.info(f\"Preparing training data with {len(df)} samples\")\n",
    "        \n",
    "        # Label=1인 positive pairs만 사용\n",
    "        positive_df = df[df['label'] == 1]\n",
    "        \n",
    "        for _, row in positive_df.iterrows():\n",
    "            # Label 없이 positive pair만 생성\n",
    "            # MultipleNegativesRankingLoss가 배치 내에서 자동으로 negative sampling 수행\n",
    "            examples.append(InputExample(texts=[row['query'], row['value']]))\n",
    "        \n",
    "        logging.info(f\"Created {len(examples)} positive pairs\")\n",
    "        logging.info(\"Negative pairs will be automatically generated in-batch by MultipleNegativesRankingLoss\")\n",
    "        \n",
    "        # 데이터 셔플\n",
    "        random.shuffle(examples)\n",
    "        return examples\n",
    "    \n",
    "  \n",
    "    def create_simple_evaluator(self, eval_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        간단한 평가자 생성 (상관계수 대신 정확도 기반)\n",
    "        \"\"\"\n",
    "        from sentence_transformers.evaluation import BinaryClassificationEvaluator\n",
    "        \n",
    "        # 평가 데이터를 binary classification 형태로 변환\n",
    "        sentences1 = []\n",
    "        sentences2 = []\n",
    "        scores = []\n",
    "        \n",
    "        queries = eval_df['query'].tolist()\n",
    "        values = eval_df['value'].tolist()\n",
    "        \n",
    "        # Positive pairs (실제 매칭)\n",
    "        for query, value in zip(queries, values):\n",
    "            sentences1.append(query)\n",
    "            sentences2.append(value)\n",
    "            scores.append(1)  # positive\n",
    "        \n",
    "        # Negative pairs (랜덤 매칭)\n",
    "        for i in range(min(len(queries), 20)):  # 최대 20개 negative\n",
    "            neg_idx = (i + len(queries) // 2) % len(values)  # 다른 인덱스 선택\n",
    "            sentences1.append(queries[i])\n",
    "            sentences2.append(values[neg_idx])\n",
    "            scores.append(0)  # negative\n",
    "        \n",
    "        logging.info(f\"Created binary evaluator with {len(sentences1)} pairs\")\n",
    "        logging.info(f\"Positive: {scores.count(1)}, Negative: {scores.count(0)}\")\n",
    "        \n",
    "        return BinaryClassificationEvaluator(\n",
    "            sentences1=sentences1,\n",
    "            sentences2=sentences2,\n",
    "            labels=scores,\n",
    "            name=\"insurance_binary_eval\"\n",
    "        )\n",
    "    \n",
    "    def train_model_with_safe_evaluator(self, \n",
    "                                       train_df: pd.DataFrame,\n",
    "                                       eval_df: pd.DataFrame = None,\n",
    "                                       epochs: int = 4,\n",
    "                                       batch_size: int = 16,\n",
    "                                       learning_rate: float = 2e-5,\n",
    "                                       warmup_steps: int = None):\n",
    "        \"\"\"\n",
    "        안전한 평가자를 사용한 학습\n",
    "        \"\"\"\n",
    "        # Sentence Transformer 준비\n",
    "        self.prepare_sentence_transformer()\n",
    "        \n",
    "        # 학습 데이터 준비 (positive pairs만)\n",
    "        train_examples = self.prepare_training_data(train_df)\n",
    "        train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)\n",
    "        \n",
    "        # Loss function 설정 (MultipleNegativesRankingLoss)\n",
    "        train_loss = losses.MultipleNegativesRankingLoss(model=self.sentence_transformer)\n",
    "        \n",
    "        # Warmup steps 계산\n",
    "        if warmup_steps is None:\n",
    "            warmup_steps = int(len(train_dataloader) * epochs * 0.1)\n",
    "        \n",
    "        # 안전한 평가자 생성\n",
    "        evaluator = None\n",
    "        if eval_df is not None and len(eval_df) >= 5:\n",
    "            try:\n",
    "                evaluator = self.create_simple_evaluator(eval_df)\n",
    "                logging.info(\"✅ Safe binary evaluator created\")\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"⚠️ Failed to create safe evaluator: {e}\")\n",
    "                evaluator = None\n",
    "        \n",
    "        # 모델 학습\n",
    "        logging.info(\"🚀 Starting training with safe evaluator...\")\n",
    "        logging.info(f\"📊 Training examples: {len(train_examples)}\")\n",
    "        logging.info(f\"📦 Batch size: {batch_size}\")\n",
    "        logging.info(f\"🔄 Epochs: {epochs}\")\n",
    "        logging.info(f\"🔥 Learning rate: {learning_rate}\")\n",
    "        logging.info(f\"⚡ Warmup steps: {warmup_steps}\")\n",
    "        logging.info(f\"📈 Safe evaluator enabled: {evaluator is not None}\")\n",
    "        \n",
    "        if evaluator is not None:\n",
    "            self.sentence_transformer.fit(\n",
    "                train_objectives=[(train_dataloader, train_loss)],\n",
    "                epochs=epochs,\n",
    "                warmup_steps=warmup_steps,\n",
    "                evaluator=evaluator,\n",
    "                evaluation_steps=35,  # 더 자주 평가 (매 에포크마다)\n",
    "                output_path=self.output_path,\n",
    "                save_best_model=True,\n",
    "                show_progress_bar=True,\n",
    "                optimizer_params={'lr': learning_rate},\n",
    "                use_amp=False,  # Mixed precision 비활성화\n",
    "                checkpoint_save_steps=None,  # 체크포인트 저장 비활성화\n",
    "                checkpoint_save_total_limit=None\n",
    "            )\n",
    "        else:\n",
    "            # Fallback: 평가자 없이 학습\n",
    "            self.sentence_transformer.fit(\n",
    "                train_objectives=[(train_dataloader, train_loss)],\n",
    "                epochs=epochs,\n",
    "                warmup_steps=warmup_steps,\n",
    "                output_path=self.output_path,\n",
    "                show_progress_bar=True,\n",
    "                optimizer_params={'lr': learning_rate}\n",
    "            )\n",
    "        \n",
    "        logging.info(f\"✅ Training completed. Saved to: {self.output_path}\")\n",
    "    \n",
    "    def load_trained_model(self, model_path: str = None):\n",
    "        \"\"\"학습된 모델 로드\"\"\"\n",
    "        if model_path is None:\n",
    "            model_path = self.output_path\n",
    "        \n",
    "        logging.info(f\"Loading trained model from: {model_path}\")\n",
    "        self.sentence_transformer = SentenceTransformer(model_path)\n",
    "    \n",
    "    def evaluate_model_performance(self, test_df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"모델 성능 평가\"\"\"\n",
    "        if self.sentence_transformer is None:\n",
    "            raise ValueError(\"Model not loaded. Please train or load a model first.\")\n",
    "        \n",
    "        # 임베딩 생성\n",
    "        queries = test_df['query'].tolist()\n",
    "        values = test_df['value'].tolist()\n",
    "        labels = test_df['label'].tolist()\n",
    "        \n",
    "        logging.info(\"🔍 Generating embeddings for evaluation...\")\n",
    "        query_embeddings = self.sentence_transformer.encode(queries, show_progress_bar=True, device='cuda')\n",
    "        value_embeddings = self.sentence_transformer.encode(values, show_progress_bar=True, device='cuda')\n",
    "        \n",
    "        # 유사도 계산\n",
    "        similarities = []\n",
    "        for q_emb, v_emb in zip(query_embeddings, value_embeddings):\n",
    "            sim = cosine_similarity([q_emb], [v_emb])[0][0]\n",
    "            similarities.append(sim)\n",
    "        \n",
    "        # 성능 지표 계산\n",
    "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "        \n",
    "        # 유사도를 이진 분류로 변환 (임계값 0.5)\n",
    "        predictions = [1 if sim > 0.5 else 0 for sim in similarities]\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(labels, predictions),\n",
    "            'precision': precision_score(labels, predictions, zero_division=0),\n",
    "            'recall': recall_score(labels, predictions, zero_division=0),\n",
    "            'f1_score': f1_score(labels, predictions, zero_division=0),\n",
    "            'avg_similarity': np.mean(similarities),\n",
    "            'std_similarity': np.std(similarities)\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def recommend_products(self, \n",
    "                          user_query: str, \n",
    "                          product_values: List[str], \n",
    "                          top_k: int = 5) -> List[Tuple[int, float]]:\n",
    "        \"\"\"사용자 쿼리에 대해 상품 추천\"\"\"\n",
    "        if self.sentence_transformer is None:\n",
    "            raise ValueError(\"Model not loaded. Please train or load a model first.\")\n",
    "        \n",
    "        # 임베딩 생성\n",
    "        query_embedding = self.sentence_transformer.encode([user_query])\n",
    "        product_embeddings = self.sentence_transformer.encode(product_values)\n",
    "        \n",
    "        # 코사인 유사도 계산\n",
    "        similarities = cosine_similarity(query_embedding, product_embeddings)[0]\n",
    "        \n",
    "        # 상위 k개 추천\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        recommendations = [(idx, similarities[idx]) for idx in top_indices]\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "\n",
    "def main_training_pipeline(result_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    메인 학습 파이프라인\n",
    "    \n",
    "    Args:\n",
    "        result_df: InsuranceDataConverter로 변환된 데이터프레임\n",
    "    \"\"\"\n",
    "    \n",
    "    logging.info(\"🎯 보험 추천 모델 학습 파이프라인 시작\")\n",
    "    \n",
    "    # 1. 데이터 전처리 및 분할\n",
    "    logging.info(\"📊 데이터 전처리 및 분할 중...\")\n",
    "    \n",
    "    # 날짜별로 분할 (시간순 분할)\n",
    "    if 'date' in result_df.columns:\n",
    "        unique_dates = sorted(result_df['date'].unique())\n",
    "        train_dates = unique_dates[:-1]  # 마지막 날짜 제외하고 학습\n",
    "        test_dates = [unique_dates[-1]]  # 마지막 날짜를 테스트\n",
    "        \n",
    "        train_df = result_df[result_df['date'].isin(train_dates)]\n",
    "        test_df = result_df[result_df['date'].isin(test_dates)]\n",
    "        \n",
    "        logging.info(f\"📅 Train dates: {train_dates}\")\n",
    "        logging.info(f\"📅 Test dates: {test_dates}\")\n",
    "    else:\n",
    "        # 날짜 정보가 없으면 랜덤 분할\n",
    "        train_df, test_df = train_test_split(result_df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # 학습/검증 분할\n",
    "    train_df, eval_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "    \n",
    "    logging.info(f\"📈 Train samples: {len(train_df)}\")\n",
    "    logging.info(f\"📊 Eval samples: {len(eval_df)}\")\n",
    "    logging.info(f\"🧪 Test samples: {len(test_df)}\")\n",
    "    \n",
    "    # 2. 트레이너 초기화\n",
    "    trainer = InsuranceEmbeddingTrainer(\n",
    "        model_path='./MiniLM-L12-v2/0_Transformer',\n",
    "        output_path='./trained_insurance_model_retoken'\n",
    "    )\n",
    "    \n",
    "    # 3. 사전 학습된 모델 로드 (선택적)\n",
    "    try:\n",
    "        trainer.load_pretrained_model()\n",
    "        \n",
    "        # 사전 학습된 모델로 샘플 임베딩 테스트\n",
    "        sample_sentences = train_df['query'].head(3).tolist()\n",
    "        embeddings = trainer.get_embeddings_manual(sample_sentences)\n",
    "        logging.info(f\"✅ 사전 학습된 모델 임베딩 테스트: {embeddings.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.warning(f\"⚠️ 사전 학습된 모델 로드 실패: {e}\")\n",
    "        logging.info(\"🌐 온라인 모델 사용으로 전환\")\n",
    "    \n",
    "    # 4. 안전한 평가자와 함께 학습\n",
    "    logging.info(\"🚀 안전한 평가자와 함께 학습 시작...\")\n",
    "    trainer.train_model_with_safe_evaluator(\n",
    "        train_df=train_df,\n",
    "        eval_df=eval_df,\n",
    "        epochs=4,\n",
    "        batch_size=16,\n",
    "        learning_rate=2e-5\n",
    "    )\n",
    "    \n",
    "    # 5. 모델 성능 평가\n",
    "    logging.info(\"📊 모델 성능 평가 중...\")\n",
    "    metrics = trainer.evaluate_model_performance(test_df)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"📊 모델 성능 평가 결과\")\n",
    "    print(\"=\"*50)\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric:15}: {value:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 6. 추천 시스템 테스트\n",
    "    logging.info(\"🎯 추천 시스템 테스트 중...\")\n",
    "    \n",
    "    # 테스트 쿼리 생성\n",
    "    test_query = test_df['query'].iloc[0]\n",
    "    test_values = test_df['value'].head(10).tolist()\n",
    "    \n",
    "    recommendations = trainer.recommend_products(\n",
    "        user_query=test_query,\n",
    "        product_values=test_values,\n",
    "        top_k=5\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"🎯 추천 시스템 테스트 결과\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"👤 사용자 쿼리: {test_query[:100]}...\")\n",
    "    print(\"\\n📋 추천 상품:\")\n",
    "    for i, (idx, score) in enumerate(recommendations):\n",
    "        print(f\"{i+1}. 유사도: {score:.4f}\")\n",
    "        print(f\"   상품: {test_values[idx][:100]}...\")\n",
    "        print()\n",
    "    \n",
    "    # 7. 모델 저장 확인\n",
    "    if os.path.exists(trainer.output_path):\n",
    "        logging.info(f\"✅ 모델이 성공적으로 저장되었습니다: {trainer.output_path}\")\n",
    "    else:\n",
    "        logging.error(f\"❌ 모델 저장 실패: {trainer.output_path}\")\n",
    "    \n",
    "    return trainer, metrics\n",
    "\n",
    "\n",
    "# 실행 예시\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    trainer, metrics = main_training_pipeline(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class AttentionAnalyzer:\n",
    "    \"\"\"보험 추천 모델의 어텐션 패턴 분석 도구\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str):\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"모델과 토크나이저 로드\"\"\"\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "        self.model = AutoModel.from_pretrained(self.model_path, output_attentions=True)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        print(f\"✅ 모델 로드 완료: {self.model_path}\")\n",
    "       \n",
    "    \n",
    "    def clean_token(self, token: str) -> str:\n",
    "        \"\"\"토큰 정리 함수\"\"\"\n",
    "        cleaned = token.replace('▁', '').replace('##', '')\n",
    "        if cleaned in ['[CLS]', '[SEP]', '[PAD]', '<s>', '</s>', '<pad>', '<unk>']:\n",
    "            return cleaned\n",
    "        if not cleaned.strip():\n",
    "            return '[SPACE]'\n",
    "        return cleaned\n",
    "    \n",
    "    def get_attention_weights(self, text: str) -> Tuple[List[str], torch.Tensor]:\n",
    "        \"\"\"텍스트에 대한 어텐션 가중치 추출\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            text, return_tensors=\"pt\", padding=True, \n",
    "            truncation=True, max_length=128\n",
    "        ).to(self.device)\n",
    "        \n",
    "        raw_tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "        print(f\"raw_tokens: {raw_tokens}\")\n",
    "        tokens = [self.clean_token(token) for token in raw_tokens]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            attention_weights = torch.stack(outputs.attentions)\n",
    "            attention_weights = attention_weights.squeeze(1)\n",
    "        \n",
    "        return tokens, attention_weights.cpu()\n",
    "    \n",
    "    def visualize_token_importance(self, text: str, layer_idx: int = -1, top_k: int = 10):\n",
    "        \"\"\"토큰 중요도 막대 그래프\"\"\"\n",
    "        tokens, attention_weights = self.get_attention_weights(text)\n",
    "        \n",
    "        if layer_idx == -1:\n",
    "            layer_idx = attention_weights.shape[0] - 1\n",
    "        attention = attention_weights[layer_idx].mean(dim=0)  # 모든 헤드 평균\n",
    "        \n",
    "        cls_attention = attention[0, :].numpy()  # CLS 토큰의 어텐션\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'token': tokens,\n",
    "            'importance': cls_attention,\n",
    "            'position': range(len(tokens))\n",
    "        })\n",
    "\n",
    "        df = df.sort_values('importance', ascending=False)\n",
    "        \n",
    "        # 특수 토큰 제외\n",
    "        special_tokens = ['[CLS]', '[SEP]', '[PAD]', '<s>', '</s>', '<pad>', '[SPACE]']\n",
    "        df_filtered = df[~df['token'].isin(special_tokens)].head(top_k)\n",
    "        \n",
    "        # 시각화\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bars = plt.bar(range(len(df_filtered)), df_filtered['importance'], \n",
    "                       color=plt.cm.viridis(df_filtered['importance'] / df_filtered['importance'].max()))\n",
    "        \n",
    "        plt.xlabel('Tokens', fontsize=12)\n",
    "        plt.ylabel('Attention Weight', fontsize=12)\n",
    "        plt.title(f'Token Importance Analysis (Layer {layer_idx})\\nText: \"{text[:50]}...\"', \n",
    "                 fontsize=14, pad=20)\n",
    "        plt.xticks(range(len(df_filtered)), df_filtered['token'], rotation=45, ha='right')\n",
    "        \n",
    "        # 값 표시\n",
    "        for i, (bar, val) in enumerate(zip(bars, df_filtered['importance'])):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "                    f'{val:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return df_filtered\n",
    "    \n",
    "    def visualize_attention_heatmap(self, text: str, layer_idx: int = -1, head_idx: int = 0):\n",
    "        \"\"\"어텐션 히트맵 시각화\"\"\"\n",
    "        tokens, attention_weights = self.get_attention_weights(text)\n",
    "        \n",
    "        if layer_idx == -1:\n",
    "            layer_idx = attention_weights.shape[0] - 1\n",
    "        \n",
    "        attention = attention_weights[layer_idx, head_idx].numpy()\n",
    "        \n",
    "        # 토큰 길이 제한\n",
    "        max_tokens = 100\n",
    "        if len(tokens) > max_tokens:\n",
    "            tokens = tokens[:max_tokens]\n",
    "            attention = attention[:max_tokens, :max_tokens]\n",
    "        \n",
    "        # 토큰 길이 제한 (표시용)\n",
    "        display_tokens = [token[:6] + '..' if len(token) > 8 else token for token in tokens]\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(\n",
    "            attention, xticklabels=display_tokens, yticklabels=display_tokens,\n",
    "            cmap='Blues', annot=False, cbar_kws={'label': 'Attention Weight'}\n",
    "        )\n",
    "        \n",
    "        plt.title(f'Attention Heatmap\\nLayer {layer_idx}, Head {head_idx}\\nText: \"{text[:50]}...\"', \n",
    "                 fontsize=14, pad=20)\n",
    "        plt.xlabel('Key Tokens', fontsize=12)\n",
    "        plt.ylabel('Query Tokens', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def analyze_test_samples(test_df: pd.DataFrame, \n",
    "                        model_path: str = \"./trained_insurance_model_retoken\",\n",
    "                        sample_size: int = 3):\n",
    "    \"\"\"\n",
    "    test_df에서 샘플을 뽑아서 어텐션 시각화\n",
    "    \n",
    "    Args:\n",
    "        test_df: 테스트 데이터프레임 (query, value 컬럼 필요)\n",
    "        model_path: 모델 경로\n",
    "        sample_size: 분석할 샘플 개수\n",
    "    \"\"\"\n",
    "    print(\"🔍 테스트 데이터 어텐션 분석 시작...\")\n",
    "    print(f\"📊 test_df 크기: {test_df.shape}\")\n",
    "    print(f\"📋 컬럼명: {list(test_df.columns)}\")\n",
    "    \n",
    "    # 분석기 생성 및 모델 로드\n",
    "    analyzer = AttentionAnalyzer(model_path)\n",
    "    analyzer.load_model()\n",
    "    \n",
    "    # 샘플 데이터 추출\n",
    "    sample_data = test_df.sample(n=min(sample_size, len(test_df)), random_state=42)\n",
    "    \n",
    "    print(f\"\\n📊 {len(sample_data)}개 샘플 분석 시작\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, (idx, row) in enumerate(sample_data.iterrows()):\n",
    "        query = str(row['query'])\n",
    "        value = str(row['value'])\n",
    "        \n",
    "        print(f\"\\n🔍 샘플 {i+1}/{len(sample_data)}\")\n",
    "        print(f\"📝 쿼리: {query}...\")\n",
    "        print(f\"🏷️ 상품: {value}...\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # 1. 쿼리 토큰 중요도 분석\n",
    "        print(\"📊 쿼리 토큰 중요도:\")\n",
    "        query_df = analyzer.visualize_token_importance(query, top_k=8)\n",
    "        query_top_tokens = query_df['token'].head(3).tolist()\n",
    "        print(f\"상위 토큰: {', '.join(query_top_tokens)}\")\n",
    "        \n",
    "        # 2. 상품 토큰 중요도 분석  \n",
    "        print(\"\\n📊 상품 토큰 중요도:\")\n",
    "        value_df = analyzer.visualize_token_importance(value, top_k=8)\n",
    "        value_top_tokens = value_df['token'].head(3).tolist()\n",
    "        print(f\"상위 토큰: {', '.join(value_top_tokens)}\")\n",
    "        \n",
    "        # 3. 쿼리 어텐션 히트맵\n",
    "        print(\"\\n📊 쿼리 어텐션 히트맵:\")\n",
    "        analyzer.visualize_attention_heatmap(query, layer_idx=-1, head_idx=0)\n",
    "        \n",
    "        results.append({\n",
    "            'index': idx,\n",
    "            'query': query,\n",
    "            'value': value,\n",
    "            'query_top_tokens': query_top_tokens,\n",
    "            'value_top_tokens': value_top_tokens,\n",
    "            'query_analysis': query_df,\n",
    "            'value_analysis': value_df\n",
    "        })\n",
    "        \n",
    "        print(f\"✅ 샘플 {i+1} 분석 완료\\n\")\n",
    "    \n",
    "    print(\"🎉 모든 샘플 분석 완료!\")\n",
    "    return analyzer, results\n",
    "\n",
    "\n",
    "# 사용 예시\n",
    "if __name__ == \"__main__\":\n",
    "    # test_df가 있다고 가정하고 사용\n",
    "    # 원래 학습 파이프라인과 정확히 동일\n",
    "    unique_dates = sorted(result_df['date'].unique())\n",
    "    train_dates = unique_dates[:-1]  # 마지막 날짜 제외\n",
    "    test_dates = [unique_dates[-1]]  # 마지막 날짜만 테스트\n",
    "\n",
    "    train_df = result_df[result_df['date'].isin(train_dates)]\n",
    "    test_df = result_df[result_df['date'].isin(test_dates)]\\\n",
    "    \n",
    "    analyzer, results = analyze_test_samples(\n",
    "        test_df=test_df, \n",
    "        model_path=\"./trained_insurance_model_retoken\",\n",
    "        sample_size=3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
